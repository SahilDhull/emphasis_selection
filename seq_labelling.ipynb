{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM_LSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, tagset_size, charset_size, char_emb_dim, char_rnn_dim, char_rnn_layers, vocab_size,\n",
    "                 lm_vocab_size, word_emb_dim, word_rnn_dim, word_rnn_layers, dropout, highway_layers=1):\n",
    "        \"\"\"\n",
    "        :param tagset_size: number of tags\n",
    "        :param charset_size: size of character vocabulary\n",
    "        :param char_emb_dim: size of character embeddings\n",
    "        :param char_rnn_dim: size of character RNNs/LSTMs\n",
    "        :param char_rnn_layers: number of layers in character RNNs/LSTMs\n",
    "        :param vocab_size: input vocabulary size\n",
    "        :param lm_vocab_size: vocabulary size of language models (in-corpus words subject to word frequency threshold)\n",
    "        :param word_emb_dim: size of word embeddings\n",
    "        :param word_rnn_dim: size of word RNN/BLSTM\n",
    "        :param word_rnn_layers:  number of layers in word RNNs/LSTMs\n",
    "        :param dropout: dropout\n",
    "        :param highway_layers: number of transform and gate layers\n",
    "        \"\"\"\n",
    "\n",
    "        super(LM_LSTM_CRF, self).__init__()\n",
    "\n",
    "        self.tagset_size = tagset_size  # this is the size of the output vocab of the tagging model\n",
    "\n",
    "        self.charset_size = charset_size\n",
    "        self.char_emb_dim = char_emb_dim\n",
    "        self.char_rnn_dim = char_rnn_dim\n",
    "        self.char_rnn_layers = char_rnn_layers\n",
    "\n",
    "        self.wordset_size = vocab_size  # this is the size of the input vocab (embedding layer) of the tagging model\n",
    "        self.lm_vocab_size = lm_vocab_size  # this is the size of the output vocab of the language model\n",
    "        self.word_emb_dim = word_emb_dim\n",
    "        self.word_rnn_dim = word_rnn_dim\n",
    "        self.word_rnn_layers = word_rnn_layers\n",
    "\n",
    "        self.highway_layers = highway_layers\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.char_embeds = nn.Embedding(self.charset_size, self.char_emb_dim)  # character embedding layer\n",
    "        self.forw_char_lstm = nn.LSTM(self.char_emb_dim, self.char_rnn_dim, num_layers=self.char_rnn_layers,\n",
    "                                      bidirectional=False, dropout=dropout)  # forward character LSTM\n",
    "        self.back_char_lstm = nn.LSTM(self.char_emb_dim, self.char_rnn_dim, num_layers=self.char_rnn_layers,\n",
    "                                      bidirectional=False, dropout=dropout)  # backward character LSTM\n",
    "\n",
    "        self.word_embeds = nn.Embedding(self.wordset_size, self.word_emb_dim)  # word embedding layer\n",
    "        self.word_blstm = nn.LSTM(self.word_emb_dim + self.char_rnn_dim * 2, self.word_rnn_dim // 2,\n",
    "                                  num_layers=self.word_rnn_layers, bidirectional=True, dropout=dropout)  # word BLSTM\n",
    "\n",
    "        self.crf = CRF((self.word_rnn_dim // 2) * 2, self.tagset_size)  # conditional random field\n",
    "\n",
    "        self.forw_lm_hw = Highway(self.char_rnn_dim, num_layers=self.highway_layers,\n",
    "                                  dropout=dropout)  # highway to transform forward char LSTM output for the forward language model\n",
    "        self.back_lm_hw = Highway(self.char_rnn_dim, num_layers=self.highway_layers,\n",
    "                                  dropout=dropout)  # highway to transform backward char LSTM output for the backward language model\n",
    "        self.subword_hw = Highway(2 * self.char_rnn_dim, num_layers=self.highway_layers,\n",
    "                                  dropout=dropout)  # highway to transform combined forward and backward char LSTM outputs for use in the word BLSTM\n",
    "\n",
    "        self.forw_lm_out = nn.Linear(self.char_rnn_dim,\n",
    "                                     self.lm_vocab_size)  # linear layer to find vocabulary scores for the forward language model\n",
    "        self.back_lm_out = nn.Linear(self.char_rnn_dim,\n",
    "                                     self.lm_vocab_size)  # linear layer to find vocabulary scores for the backward language model\n",
    "\n",
    "    def init_word_embeddings(self, embeddings):\n",
    "        \"\"\"\n",
    "        Initialize embeddings with pre-trained embeddings.\n",
    "        :param embeddings: pre-trained embeddings\n",
    "        \"\"\"\n",
    "        self.word_embeds.weight = nn.Parameter(embeddings)\n",
    "\n",
    "    def fine_tune_word_embeddings(self, fine_tune=False):\n",
    "        \"\"\"\n",
    "        Fine-tune embedding layer? (Not fine-tuning only makes sense if using pre-trained embeddings).\n",
    "        :param fine_tune: Fine-tune?\n",
    "        \"\"\"\n",
    "        for p in self.word_embeds.parameters():\n",
    "            p.requires_grad = fine_tune\n",
    "\n",
    "    def forward(self, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, wmaps, tmaps, wmap_lengths, cmap_lengths, probs):\n",
    "        \n",
    "        self.batch_size = cmaps_f.size(0)\n",
    "        self.word_pad_len = wmaps.size(1)\n",
    "\n",
    "        # Sort by decreasing true char. sequence length\n",
    "        cmap_lengths, char_sort_ind = cmap_lengths.sort(dim=0, descending=True)\n",
    "        cmaps_f = cmaps_f[char_sort_ind]\n",
    "        cmaps_b = cmaps_b[char_sort_ind]\n",
    "        cmarkers_f = cmarkers_f[char_sort_ind]\n",
    "        cmarkers_b = cmarkers_b[char_sort_ind]\n",
    "        probs = probs\n",
    "        \n",
    "        wmaps = wmaps[char_sort_ind]\n",
    "        tmaps = tmaps[char_sort_ind]\n",
    "        wmap_lengths = wmap_lengths[char_sort_ind]\n",
    "\n",
    "        # Embedding look-up for characters\n",
    "        cf = self.char_embeds(cmaps_f)  # (batch_size, char_pad_len, char_emb_dim)\n",
    "        cb = self.char_embeds(cmaps_b)\n",
    "\n",
    "        # Dropout\n",
    "        cf = self.dropout(cf)  # (batch_size, char_pad_len, char_emb_dim)\n",
    "        cb = self.dropout(cb)\n",
    "\n",
    "        # Pack padded sequence\n",
    "        cf = pack_padded_sequence(cf, cmap_lengths.tolist(),\n",
    "                                  batch_first=True)  # packed sequence of char_emb_dim, with real sequence lengths\n",
    "        cb = pack_padded_sequence(cb, cmap_lengths.tolist(), batch_first=True)\n",
    "\n",
    "        # LSTM\n",
    "        cf, _ = self.forw_char_lstm(cf)  # packed sequence of char_rnn_dim, with real sequence lengths\n",
    "        cb, _ = self.back_char_lstm(cb)\n",
    "\n",
    "        # Unpack packed sequence\n",
    "        cf, _ = pad_packed_sequence(cf, batch_first=True)  # (batch_size, max_char_len_in_batch, char_rnn_dim)\n",
    "        cb, _ = pad_packed_sequence(cb, batch_first=True)\n",
    "\n",
    "        # Sanity check\n",
    "        assert cf.size(1) == max(cmap_lengths.tolist()) == list(cmap_lengths)[0]\n",
    "\n",
    "        # Select RNN outputs only at marker points (spaces in the character sequence)\n",
    "        cmarkers_f = cmarkers_f.unsqueeze(2).expand(self.batch_size, self.word_pad_len, self.char_rnn_dim)\n",
    "        cmarkers_b = cmarkers_b.unsqueeze(2).expand(self.batch_size, self.word_pad_len, self.char_rnn_dim)\n",
    "        cf_selected = torch.gather(cf, 1, cmarkers_f)  # (batch_size, word_pad_len, char_rnn_dim)\n",
    "        cb_selected = torch.gather(cb, 1, cmarkers_b)\n",
    "\n",
    "        # Only for co-training, not useful for tagging after model is trained\n",
    "        if self.training:\n",
    "            lm_f = self.forw_lm_hw(self.dropout(cf_selected))  # (batch_size, word_pad_len, char_rnn_dim)\n",
    "            lm_b = self.back_lm_hw(self.dropout(cb_selected))\n",
    "            lm_f_scores = self.forw_lm_out(self.dropout(lm_f))  # (batch_size, word_pad_len, lm_vocab_size)\n",
    "            lm_b_scores = self.back_lm_out(self.dropout(lm_b))\n",
    "\n",
    "        # Sort by decreasing true word sequence length\n",
    "        wmap_lengths, word_sort_ind = wmap_lengths.sort(dim=0, descending=True)\n",
    "        wmaps = wmaps[word_sort_ind]\n",
    "        tmaps = tmaps[word_sort_ind]\n",
    "        cf_selected = cf_selected[word_sort_ind]  # for language model\n",
    "        cb_selected = cb_selected[word_sort_ind]\n",
    "        if self.training:\n",
    "            lm_f_scores = lm_f_scores[word_sort_ind]\n",
    "            lm_b_scores = lm_b_scores[word_sort_ind]\n",
    "\n",
    "        # Embedding look-up for words\n",
    "        w = self.word_embeds(wmaps)  # (batch_size, word_pad_len, word_emb_dim)\n",
    "        w = self.dropout(w)\n",
    "\n",
    "        # Sub-word information at each word\n",
    "        subword = self.subword_hw(self.dropout(\n",
    "            torch.cat((cf_selected, cb_selected), dim=2)))  # (batch_size, word_pad_len, 2 * char_rnn_dim)\n",
    "        subword = self.dropout(subword)\n",
    "\n",
    "        # Concatenate word embeddings and sub-word features\n",
    "        w = torch.cat((w, subword), dim=2)  # (batch_size, word_pad_len, word_emb_dim + 2 * char_rnn_dim)\n",
    "\n",
    "        # Pack padded sequence\n",
    "        w = pack_padded_sequence(w, list(wmap_lengths),\n",
    "                                 batch_first=True)  # packed sequence of word_emb_dim + 2 * char_rnn_dim, with real sequence lengths\n",
    "\n",
    "        # LSTM\n",
    "        w, _ = self.word_blstm(w)  # packed sequence of word_rnn_dim, with real sequence lengths\n",
    "\n",
    "        # Unpack packed sequence\n",
    "        w, _ = pad_packed_sequence(w, batch_first=True)  # (batch_size, max_word_len_in_batch, word_rnn_dim)\n",
    "        w = self.dropout(w)\n",
    "\n",
    "        crf_scores = self.crf(w)  # (batch_size, max_word_len_in_batch, tagset_size, tagset_size)\n",
    "\n",
    "        if self.training:\n",
    "            return crf_scores, lm_f_scores, lm_b_scores, wmaps, tmaps, wmap_lengths, word_sort_ind, char_sort_ind\n",
    "        else:\n",
    "            return crf_scores, wmaps, tmaps, wmap_lengths, word_sort_ind, char_sort_ind  # sort inds to reorder, if req."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
