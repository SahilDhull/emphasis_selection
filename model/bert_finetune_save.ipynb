{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_finetune_save.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMXGvNNmepzZtP6/+7UoK7k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SahilDhull/emphasis_selection/blob/master/model/bert_finetune_save.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoSzRlf9cr-k",
        "colab_type": "code",
        "outputId": "b7c322d0-9b22-4ff6-fed7-0c586a64e832",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "!pip install transformers\n",
        "!pip install config"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: config in /usr/local/lib/python3.6/dist-packages (0.4.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Z_k_Gv6dSGv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "9dcdf1ce-7b74-4198-a969-8e260fb848f5"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "from transformers import BertForMaskedLM , BertModel ,WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import PreTrainedModel, PreTrainedTokenizer , BertPreTrainedModel\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import codecs\n",
        "import os"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcU3hGuCd0GX",
        "colab_type": "code",
        "outputId": "390d2991-1f43-43fa-f2c7-c12808d22abc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla P4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpuZV36Ad7aJ",
        "colab_type": "code",
        "outputId": "6fdf0852-3cfa-4010-f8b1-63a1a8d90143",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "quotes_file = 'drive/My Drive/datasets/all_quotes.txt'\n",
        "path = 'drive/My Drive/datasets/bert_finetune/'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdW-iS47eEZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jq23Im6eXvt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_sent(file, caseless = False):\n",
        "    \n",
        "    with codecs.open(file, 'r', 'utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "    #print(lines)\n",
        "    sent = \"\"\n",
        "    sents = []\n",
        "    \n",
        "    for line in lines:\n",
        "        if not (line.isspace()):\n",
        "            feats = line.strip().split()\n",
        "            word = feats[0].lower() if caseless else feats[0]\n",
        "            if(word == \"n't\"):\n",
        "              word = \"'t\"\n",
        "              sent = sent + \"n\"\n",
        "            sent = sent + \" \" + word\n",
        "        elif len(sent) > 0:\n",
        "            sents.append(sent.strip())\n",
        "            sent = \"\"\n",
        "            \n",
        "    if len(sent) > 0:\n",
        "        sents.append(sent)\n",
        "    \n",
        "    return sents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGXoN9isekYj",
        "colab_type": "code",
        "outputId": "ef6ca7f1-5e74-4aef-ce3c-544211ffda88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "sentences = read_sent(quotes_file)\n",
        "print(sentences[0])\n",
        "print(sentences[100])\n",
        "\n",
        "sentences = [\"[CLS] \" + query + \" [SEP]\" for query in sentences]\n",
        "print(sentences[0])\n",
        "print(sentences[100])\n",
        "\n",
        "# Tokenize with BERT tokenizer\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "\n",
        "print (tokenized_texts[0])\n",
        "print (tokenized_texts[100])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You know you 're in love when you can 't fall asleep because reality is finally better than your dreams .\n",
            "A half-read book is a half-finished love affair .\n",
            "[CLS] You know you 're in love when you can 't fall asleep because reality is finally better than your dreams . [SEP]\n",
            "[CLS] A half-read book is a half-finished love affair . [SEP]\n",
            "['[CLS]', 'You', 'know', 'you', \"'\", 're', 'in', 'love', 'when', 'you', 'can', \"'\", 't', 'fall', 'asleep', 'because', 'reality', 'is', 'finally', 'better', 'than', 'your', 'dreams', '.', '[SEP]']\n",
            "['[CLS]', 'A', 'half', '-', 'read', 'book', 'is', 'a', 'half', '-', 'finished', 'love', 'affair', '.', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWo2i7VmfJgL",
        "colab_type": "code",
        "outputId": "747a64c1-a9d6-4ad1-eeed-1361f77e9af8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "MAX_LEN = 72\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "print(input_ids[0])\n",
        "print(input_ids[100])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 101 1192 1221 1128  112 1231 1107 1567 1165 1128 1169  112  189 2303\n",
            " 6153 1272 3958 1110 1921 1618 1190 1240 6149  119  102    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[ 101  138 1544  118 2373 1520 1110  170 1544  118 1845 1567 7033  119\n",
            "  102    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8qFaQr8fKPP",
        "colab_type": "code",
        "outputId": "7d88b9bf-dd03-4c02-c33c-3f6b80588b8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "attention_masks = []\n",
        "\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)\n",
        "print(attention_masks[0])\n",
        "print(attention_masks[100])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SQT6Q2tfcpH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_inputs, validation_inputs = train_test_split(input_ids, random_state=2018, test_size=0.1)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2018, test_size=0.1)\n",
        "                                             \n",
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1KIFVCAfsfr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mask_tokens(inputs, tokenizer, mlm_probability = 0.15):\n",
        "    \"\"\" Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. \"\"\"\n",
        "    labels = inputs.clone()\n",
        "    # print(inputs[0])\n",
        "\n",
        "    # We sample a few tokens in each sequence for masked-LM training (with probability mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
        "    probability_matrix = torch.full(labels.shape, mlm_probability)\n",
        "    special_tokens_mask = [tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()]\n",
        "    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
        " \n",
        "    if tokenizer._pad_token is not None:\n",
        "        padding_mask = labels.eq(tokenizer.pad_token_id)\n",
        "        probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
        "\n",
        "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
        "    labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
        "\n",
        "    # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
        "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
        "    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
        "\n",
        "    # 10% of the time, we replace masked input tokens with random word\n",
        "    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
        "    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
        "    inputs[indices_random] = random_words[indices_random]\n",
        "\n",
        "    # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
        "    return inputs, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyUS8NftfwWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader \n",
        "train_data = TensorDataset(train_inputs, train_masks)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehHtUXK6f_SL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class bert_model(nn.Module):\n",
        "  def __init__(self, model_path):\n",
        "    super(bert_model, self).__init__()\n",
        "    self.config = BertConfig.from_pretrained(model_path, output_hidden_states=True)\n",
        "    self.bert = BertForMaskedLM.from_pretrained(model_path, config = self.config).to(device)\n",
        "    \n",
        "  def save_pretrained(self, output_dir):\n",
        "    self.config.save_pretrained(output_dir)\n",
        "    self.bert.save_pretrained(output_dir)\n",
        "\n",
        "  # def from_pretrained(self, path_dir):\n",
        "  #   self.config = BertConfig.from_pretrained(path_dir)\n",
        "  #   self.bert = BertForMaskedLM.from_pretrained(path_dir).to(device)\n",
        "           \n",
        "  def forward(self, bert_ids, bert_mask, labels = None):\n",
        "    output = self.bert(bert_ids, attention_mask = bert_mask, masked_lm_labels=labels)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFcNh4IThPfI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_epochs = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSh9QXmShikI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def evaluate(model, tokenizer, validation_dataloader):\n",
        "  total_loss = 0\n",
        "  steps = len(validation_dataloader)\n",
        "  model.eval()\n",
        "  \n",
        "  corrects = 0\n",
        "  errors = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i, (val_inputs, val_masks) in enumerate(validation_dataloader):\n",
        "      inputs, labels = mask_tokens(val_inputs, tokenizer)\n",
        "      \n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      val_masks = val_masks.to(device)\n",
        "      \n",
        "      output = model(inputs,val_masks)\n",
        "      predict = output[0]\n",
        "\n",
        "      batch_size = predict.size()[0]\n",
        "\n",
        "      for bs in range(batch_size):\n",
        "        ii = 0\n",
        "        for ls in labels[bs]:\n",
        "          ls = ls.item()\n",
        "          if ls!=-100:\n",
        "            predicted = torch.argmax(predict[bs][ii]).item()\n",
        "            if(ls == predicted):\n",
        "              corrects = corrects + 1\n",
        "            else:\n",
        "              errors = errors + 1\n",
        "          ii = ii + 1\n",
        "\n",
        "  total = corrects + errors\n",
        "  accuracy = corrects/total\n",
        "  print(\"\\nvalidation accuracy = \",accuracy,\"\\t for\",total,\"masks on validation data\\n\")\n",
        "\n",
        "  return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9pVcLKIhnoe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_dataloader,validation_dataloader, tokenizer, max_epochs, save_path, device, new_model = False, print_freq = 30,val_freq = 333):\n",
        "  \n",
        "  checkpoint_dir = os.path.join(save_path,\"last\")\n",
        "  bestpoint_dir = os.path.join(save_path,\"best\")\n",
        "  os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "  os.makedirs(bestpoint_dir, exist_ok=True)\n",
        "\n",
        "  max_accuracy = 0\n",
        "\n",
        "  if(new_model == False):\n",
        "    try:\n",
        "      with open(os.path.join(checkpoint_dir, \"epoch_trained.txt\"), 'r') as f:\n",
        "        epochs_trained = int(f.read().strip())\n",
        "      print(\"Continuing training from checkpoint\")\n",
        "      print(\"Continuing training from epoch \", epochs_trained)\n",
        "      model = bert_model(checkpoint_dir).to(device)    \n",
        "    except:\n",
        "      model = bert_model('bert-base-cased').to(device)\n",
        "      print(\"No saved checkpoint found\")\n",
        "  else:\n",
        "    model = bert_model('bert-base-cased').to(device)\n",
        "    print(\"Starting fine tuning...\")\n",
        "\n",
        "  max_steps = len(train_dataloader)*max_epochs\n",
        "  no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "  optimizer_grouped_parameters = [{\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}]\n",
        "\n",
        "  optimizer = AdamW(optimizer_grouped_parameters,lr = 1e-4)\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=max_steps/20, num_training_steps=max_steps)\n",
        "\n",
        "  if(new_model == False):\n",
        "    try:\n",
        "      # Load in optimizer and scheduler states\n",
        "      optimizer.load_state_dict(torch.load(os.path.join(save_path, \"optimizer.pt\")))\n",
        "      scheduler.load_state_dict(torch.load(os.path.join(save_path, \"scheduler.pt\")))\n",
        "    except:\n",
        "      print(\"Starting fine tuning...\")\n",
        "  \n",
        "  model.zero_grad()\n",
        "  steps = len(train_dataloader)\n",
        "\n",
        "  for epoch in range(max_epochs):    \n",
        "    accuracy =  evaluate(model, tokenizer, validation_dataloader)\n",
        "    if(accuracy > max_accuracy):\n",
        "      max_accuracy = accuracy\n",
        "      model.save_pretrained(bestpoint_dir)\n",
        "      print(\"Saving model bestpoint to \", bestpoint_dir)\n",
        "\n",
        "    total_loss = 0 \n",
        "    for i, (train_inputs, train_masks) in enumerate(train_dataloader):\n",
        "      inputs, labels = mask_tokens(train_inputs, tokenizer)\n",
        "      model.train()\n",
        "      \n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      train_masks = train_masks.to(device)\n",
        "      \n",
        "      output = model(inputs,train_masks,labels)\n",
        "      loss = output[0]\n",
        "      total_loss = total_loss + loss\n",
        "      \n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      model.zero_grad()\n",
        "\n",
        "      if((i+1)%print_freq==0):\n",
        "        avg_loss = total_loss/print_freq\n",
        "        total_loss = 0\n",
        "        print(\"epoch:\",(epoch+1),\"out of\",max_epochs,\"\\t batch:\",(i+1),\"out of\",steps,\"\\t average loss:\",avg_loss)   \n",
        "      \n",
        "      if((i+1)%val_freq==0):\n",
        "        accuracy =  evaluate(model, tokenizer, validation_dataloader)\n",
        "        if(accuracy > max_accuracy):\n",
        "          max_accuracy = accuracy\n",
        "          model.save_pretrained(bestpoint_dir)  \n",
        "          print(\"Saving model bestpoint to \", bestpoint_dir)\n",
        "        model.save_pretrained(checkpoint_dir)\n",
        "        with open(os.path.join(checkpoint_dir, \"epoch_trained.txt\"), 'w') as f:\n",
        "          f.write(str(epoch))\n",
        "        tokenizer.save_pretrained(checkpoint_dir)\n",
        "        print(\"Saving model checkpoint to \", checkpoint_dir)\n",
        "        torch.save(optimizer.state_dict(), os.path.join(save_path, \"optimizer.pt\"))\n",
        "        torch.save(scheduler.state_dict(), os.path.join(save_path, \"scheduler.pt\"))\n",
        "        print(\"Saving optimizer and scheduler states to \", save_path)\n",
        "\n",
        "    model.save_pretrained(checkpoint_dir)\n",
        "    with open(os.path.join(checkpoint_dir, \"epoch_trained.txt\"), 'w') as f:\n",
        "      f.write(str(epoch))\n",
        "    tokenizer.save_pretrained(checkpoint_dir)\n",
        "    print(\"Saving model checkpoint to \", checkpoint_dir)\n",
        "    torch.save(optimizer.state_dict(), os.path.join(save_path, \"optimizer.pt\"))\n",
        "    torch.save(scheduler.state_dict(), os.path.join(save_path, \"scheduler.pt\"))\n",
        "    print(\"Saving optimizer and scheduler states to \", save_path)\n",
        "  \n",
        "  accuracy = evaluate(model, tokenizer, validation_dataloader)\n",
        "  if(accuracy > max_accuracy):\n",
        "    max_accuracy = accuracy\n",
        "    model.save_pretrained(bestpoint_dir)\n",
        "    print(\"Saving model bestpoint to \", bestpoint_dir)\n",
        "  model.save_pretrained(checkpoint_dir)\n",
        "  with open(os.path.join(checkpoint_dir, \"epoch_trained.txt\"), 'w') as f:\n",
        "    f.write(str(epoch))\n",
        "  tokenizer.save_pretrained(checkpoint_dir)\n",
        "  print(\"Saving model checkpoint to \", checkpoint_dir)\n",
        "  torch.save(optimizer.state_dict(), os.path.join(save_path, \"optimizer.pt\"))\n",
        "  torch.save(scheduler.state_dict(), os.path.join(save_path, \"scheduler.pt\"))\n",
        "  print(\"Saving optimizer and scheduler states to \", save_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANAew19uhraM",
        "colab_type": "code",
        "outputId": "1b3fa470-f9ab-4792-e413-add53344b3e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train(train_dataloader,validation_dataloader, tokenizer, max_epochs, path, device)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Continuing training from checkpoint\n",
            "Continuing training from epoch  1\n",
            "\n",
            "validation accuracy =  0.5757831225721134 \t for 60495 masks on validation data\n",
            "\n",
            "Saving model bestpoint to  drive/My Drive/datasets/bert_finetune/best\n",
            "epoch: 1 out of 4 \t batch: 30 out of 6633 \t average loss: tensor(2.5393, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 60 out of 6633 \t average loss: tensor(2.3958, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 90 out of 6633 \t average loss: tensor(2.5151, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 120 out of 6633 \t average loss: tensor(2.4128, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 150 out of 6633 \t average loss: tensor(2.5073, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 180 out of 6633 \t average loss: tensor(2.4884, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 210 out of 6633 \t average loss: tensor(2.4195, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 240 out of 6633 \t average loss: tensor(2.4465, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 270 out of 6633 \t average loss: tensor(2.5197, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 300 out of 6633 \t average loss: tensor(2.3897, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 330 out of 6633 \t average loss: tensor(2.4088, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "validation accuracy =  0.5814206389744773 \t for 60691 masks on validation data\n",
            "\n",
            "Saving model bestpoint to  drive/My Drive/datasets/bert_finetune/best\n",
            "Saving model checkpoint to  drive/My Drive/datasets/bert_finetune/last\n",
            "Saving optimizer and scheduler states to  drive/My Drive/datasets/bert_finetune/\n",
            "epoch: 1 out of 4 \t batch: 360 out of 6633 \t average loss: tensor(2.3924, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 390 out of 6633 \t average loss: tensor(2.4339, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 420 out of 6633 \t average loss: tensor(2.4884, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 450 out of 6633 \t average loss: tensor(2.3145, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 480 out of 6633 \t average loss: tensor(2.3960, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 510 out of 6633 \t average loss: tensor(2.5227, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 540 out of 6633 \t average loss: tensor(2.4053, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 570 out of 6633 \t average loss: tensor(2.4313, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 600 out of 6633 \t average loss: tensor(2.4659, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 630 out of 6633 \t average loss: tensor(2.3305, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 660 out of 6633 \t average loss: tensor(2.4951, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "validation accuracy =  0.5752364574376613 \t for 60476 masks on validation data\n",
            "\n",
            "Saving model checkpoint to  drive/My Drive/datasets/bert_finetune/last\n",
            "Saving optimizer and scheduler states to  drive/My Drive/datasets/bert_finetune/\n",
            "epoch: 1 out of 4 \t batch: 690 out of 6633 \t average loss: tensor(2.4455, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 720 out of 6633 \t average loss: tensor(2.2506, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 750 out of 6633 \t average loss: tensor(2.4001, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 780 out of 6633 \t average loss: tensor(2.4608, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 810 out of 6633 \t average loss: tensor(2.3541, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 840 out of 6633 \t average loss: tensor(2.4358, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 870 out of 6633 \t average loss: tensor(2.3190, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 900 out of 6633 \t average loss: tensor(2.3619, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 930 out of 6633 \t average loss: tensor(2.5331, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 960 out of 6633 \t average loss: tensor(2.4130, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 990 out of 6633 \t average loss: tensor(2.4321, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "validation accuracy =  0.5780435851220711 \t for 61030 masks on validation data\n",
            "\n",
            "Saving model checkpoint to  drive/My Drive/datasets/bert_finetune/last\n",
            "Saving optimizer and scheduler states to  drive/My Drive/datasets/bert_finetune/\n",
            "epoch: 1 out of 4 \t batch: 1020 out of 6633 \t average loss: tensor(2.3584, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 1050 out of 6633 \t average loss: tensor(2.3729, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 1080 out of 6633 \t average loss: tensor(2.3915, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 1110 out of 6633 \t average loss: tensor(2.3794, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 1140 out of 6633 \t average loss: tensor(2.4207, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 1170 out of 6633 \t average loss: tensor(2.4038, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 1200 out of 6633 \t average loss: tensor(2.4226, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 1230 out of 6633 \t average loss: tensor(2.3730, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 1260 out of 6633 \t average loss: tensor(2.4437, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 1290 out of 6633 \t average loss: tensor(2.4968, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 1320 out of 6633 \t average loss: tensor(2.3558, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "validation accuracy =  0.5783889659675182 \t for 61265 masks on validation data\n",
            "\n",
            "Saving model checkpoint to  drive/My Drive/datasets/bert_finetune/last\n",
            "Saving optimizer and scheduler states to  drive/My Drive/datasets/bert_finetune/\n",
            "epoch: 1 out of 4 \t batch: 1350 out of 6633 \t average loss: tensor(2.5034, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 1380 out of 6633 \t average loss: tensor(2.5384, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 1410 out of 6633 \t average loss: tensor(2.2729, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 1440 out of 6633 \t average loss: tensor(2.4722, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 1470 out of 6633 \t average loss: tensor(2.4069, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 1500 out of 6633 \t average loss: tensor(2.3154, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 1530 out of 6633 \t average loss: tensor(2.4633, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 1560 out of 6633 \t average loss: tensor(2.3807, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 1590 out of 6633 \t average loss: tensor(2.4218, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 1620 out of 6633 \t average loss: tensor(2.3713, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 1650 out of 6633 \t average loss: tensor(2.3757, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "validation accuracy =  0.5797479570739391 \t for 60942 masks on validation data\n",
            "\n",
            "Saving model checkpoint to  drive/My Drive/datasets/bert_finetune/last\n",
            "Saving optimizer and scheduler states to  drive/My Drive/datasets/bert_finetune/\n",
            "epoch: 1 out of 4 \t batch: 1680 out of 6633 \t average loss: tensor(2.2802, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 1710 out of 6633 \t average loss: tensor(2.4762, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 1740 out of 6633 \t average loss: tensor(2.4033, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 1770 out of 6633 \t average loss: tensor(2.4644, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 1800 out of 6633 \t average loss: tensor(2.4382, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 1830 out of 6633 \t average loss: tensor(2.5307, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 1860 out of 6633 \t average loss: tensor(2.3729, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 1890 out of 6633 \t average loss: tensor(2.3892, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 1920 out of 6633 \t average loss: tensor(2.3925, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 1950 out of 6633 \t average loss: tensor(2.3763, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 1980 out of 6633 \t average loss: tensor(2.4307, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "validation accuracy =  0.5772799947254776 \t for 60669 masks on validation data\n",
            "\n",
            "Saving model checkpoint to  drive/My Drive/datasets/bert_finetune/last\n",
            "Saving optimizer and scheduler states to  drive/My Drive/datasets/bert_finetune/\n",
            "epoch: 1 out of 4 \t batch: 2010 out of 6633 \t average loss: tensor(2.4024, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 2040 out of 6633 \t average loss: tensor(2.4391, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 2070 out of 6633 \t average loss: tensor(2.3316, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 2100 out of 6633 \t average loss: tensor(2.4435, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 2130 out of 6633 \t average loss: tensor(2.3686, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 2160 out of 6633 \t average loss: tensor(2.3058, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 2190 out of 6633 \t average loss: tensor(2.3467, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 2220 out of 6633 \t average loss: tensor(2.3943, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 2250 out of 6633 \t average loss: tensor(2.3677, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 2280 out of 6633 \t average loss: tensor(2.3529, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 2310 out of 6633 \t average loss: tensor(2.3823, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "validation accuracy =  0.5784718679546464 \t for 60767 masks on validation data\n",
            "\n",
            "Saving model checkpoint to  drive/My Drive/datasets/bert_finetune/last\n",
            "Saving optimizer and scheduler states to  drive/My Drive/datasets/bert_finetune/\n",
            "epoch: 1 out of 4 \t batch: 2340 out of 6633 \t average loss: tensor(2.3865, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 2370 out of 6633 \t average loss: tensor(2.3536, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 2400 out of 6633 \t average loss: tensor(2.2801, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 2430 out of 6633 \t average loss: tensor(2.6296, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 2460 out of 6633 \t average loss: tensor(2.3543, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 2490 out of 6633 \t average loss: tensor(2.3551, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 2520 out of 6633 \t average loss: tensor(2.3098, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 2550 out of 6633 \t average loss: tensor(2.5385, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 2580 out of 6633 \t average loss: tensor(2.4504, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 2610 out of 6633 \t average loss: tensor(2.2390, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 2640 out of 6633 \t average loss: tensor(2.4572, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "validation accuracy =  0.5757470866505134 \t for 60669 masks on validation data\n",
            "\n",
            "Saving model checkpoint to  drive/My Drive/datasets/bert_finetune/last\n",
            "Saving optimizer and scheduler states to  drive/My Drive/datasets/bert_finetune/\n",
            "epoch: 1 out of 4 \t batch: 2670 out of 6633 \t average loss: tensor(2.4737, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 2700 out of 6633 \t average loss: tensor(2.4320, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 2730 out of 6633 \t average loss: tensor(2.3410, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 2760 out of 6633 \t average loss: tensor(2.4085, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 2790 out of 6633 \t average loss: tensor(2.4172, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 2820 out of 6633 \t average loss: tensor(2.3531, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 2850 out of 6633 \t average loss: tensor(2.4672, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 2880 out of 6633 \t average loss: tensor(2.4224, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 2910 out of 6633 \t average loss: tensor(2.4044, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 2940 out of 6633 \t average loss: tensor(2.3373, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 2970 out of 6633 \t average loss: tensor(2.3870, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "validation accuracy =  0.5800455610947869 \t for 60578 masks on validation data\n",
            "\n",
            "Saving model checkpoint to  drive/My Drive/datasets/bert_finetune/last\n",
            "Saving optimizer and scheduler states to  drive/My Drive/datasets/bert_finetune/\n",
            "epoch: 1 out of 4 \t batch: 3000 out of 6633 \t average loss: tensor(2.4282, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 3030 out of 6633 \t average loss: tensor(2.3404, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 3060 out of 6633 \t average loss: tensor(2.4941, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 3090 out of 6633 \t average loss: tensor(2.3236, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 3120 out of 6633 \t average loss: tensor(2.4059, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 3150 out of 6633 \t average loss: tensor(2.3276, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 3180 out of 6633 \t average loss: tensor(2.4622, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 3210 out of 6633 \t average loss: tensor(2.3659, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 3240 out of 6633 \t average loss: tensor(2.4070, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 3270 out of 6633 \t average loss: tensor(2.4815, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 3300 out of 6633 \t average loss: tensor(2.3008, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 3330 out of 6633 \t average loss: tensor(2.3946, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "validation accuracy =  0.5814430957738613 \t for 60883 masks on validation data\n",
            "\n",
            "Saving model bestpoint to  drive/My Drive/datasets/bert_finetune/best\n",
            "Saving model checkpoint to  drive/My Drive/datasets/bert_finetune/last\n",
            "Saving optimizer and scheduler states to  drive/My Drive/datasets/bert_finetune/\n",
            "epoch: 1 out of 4 \t batch: 3360 out of 6633 \t average loss: tensor(2.3163, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 3390 out of 6633 \t average loss: tensor(2.4389, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 3420 out of 6633 \t average loss: tensor(2.4067, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 3450 out of 6633 \t average loss: tensor(2.4406, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 3480 out of 6633 \t average loss: tensor(2.4051, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 3510 out of 6633 \t average loss: tensor(2.5059, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 3540 out of 6633 \t average loss: tensor(2.3401, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 3570 out of 6633 \t average loss: tensor(2.4948, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 3600 out of 6633 \t average loss: tensor(2.3578, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 3630 out of 6633 \t average loss: tensor(2.3719, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 3660 out of 6633 \t average loss: tensor(2.5280, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "validation accuracy =  0.5786936571305514 \t for 60367 masks on validation data\n",
            "\n",
            "Saving model checkpoint to  drive/My Drive/datasets/bert_finetune/last\n",
            "Saving optimizer and scheduler states to  drive/My Drive/datasets/bert_finetune/\n",
            "epoch: 1 out of 4 \t batch: 3690 out of 6633 \t average loss: tensor(2.4369, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 3720 out of 6633 \t average loss: tensor(2.2835, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 3750 out of 6633 \t average loss: tensor(2.5143, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 3780 out of 6633 \t average loss: tensor(2.4319, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 3810 out of 6633 \t average loss: tensor(2.4352, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 3840 out of 6633 \t average loss: tensor(2.4428, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 3870 out of 6633 \t average loss: tensor(2.3687, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 3900 out of 6633 \t average loss: tensor(2.4059, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 3930 out of 6633 \t average loss: tensor(2.3710, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 3960 out of 6633 \t average loss: tensor(2.3248, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 3990 out of 6633 \t average loss: tensor(2.4721, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "validation accuracy =  0.5817865717360708 \t for 60395 masks on validation data\n",
            "\n",
            "Saving model bestpoint to  drive/My Drive/datasets/bert_finetune/best\n",
            "Saving model checkpoint to  drive/My Drive/datasets/bert_finetune/last\n",
            "Saving optimizer and scheduler states to  drive/My Drive/datasets/bert_finetune/\n",
            "epoch: 1 out of 4 \t batch: 4020 out of 6633 \t average loss: tensor(2.3954, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 4050 out of 6633 \t average loss: tensor(2.4725, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 4080 out of 6633 \t average loss: tensor(2.3299, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 4110 out of 6633 \t average loss: tensor(2.4091, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 4140 out of 6633 \t average loss: tensor(2.3851, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 4170 out of 6633 \t average loss: tensor(2.3785, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 4200 out of 6633 \t average loss: tensor(2.3560, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 4230 out of 6633 \t average loss: tensor(2.4677, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 4260 out of 6633 \t average loss: tensor(2.3036, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 4290 out of 6633 \t average loss: tensor(2.3708, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 4320 out of 6633 \t average loss: tensor(2.4464, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "validation accuracy =  0.5844049067648897 \t for 60814 masks on validation data\n",
            "\n",
            "Saving model bestpoint to  drive/My Drive/datasets/bert_finetune/best\n",
            "Saving model checkpoint to  drive/My Drive/datasets/bert_finetune/last\n",
            "Saving optimizer and scheduler states to  drive/My Drive/datasets/bert_finetune/\n",
            "epoch: 1 out of 4 \t batch: 4350 out of 6633 \t average loss: tensor(2.3313, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 4380 out of 6633 \t average loss: tensor(2.3897, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 4410 out of 6633 \t average loss: tensor(2.5056, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 4440 out of 6633 \t average loss: tensor(2.4159, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 4470 out of 6633 \t average loss: tensor(2.3898, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 4500 out of 6633 \t average loss: tensor(2.4646, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 4530 out of 6633 \t average loss: tensor(2.4971, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 4560 out of 6633 \t average loss: tensor(2.4669, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 4590 out of 6633 \t average loss: tensor(2.4361, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 4620 out of 6633 \t average loss: tensor(2.3395, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 4650 out of 6633 \t average loss: tensor(2.4188, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "validation accuracy =  0.5763908407043069 \t for 60485 masks on validation data\n",
            "\n",
            "Saving model checkpoint to  drive/My Drive/datasets/bert_finetune/last\n",
            "Saving optimizer and scheduler states to  drive/My Drive/datasets/bert_finetune/\n",
            "epoch: 1 out of 4 \t batch: 4680 out of 6633 \t average loss: tensor(2.4179, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 4710 out of 6633 \t average loss: tensor(2.2842, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 4740 out of 6633 \t average loss: tensor(2.3392, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 4770 out of 6633 \t average loss: tensor(2.2959, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 4800 out of 6633 \t average loss: tensor(2.3780, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 4830 out of 6633 \t average loss: tensor(2.4759, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 4860 out of 6633 \t average loss: tensor(2.4018, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 4890 out of 6633 \t average loss: tensor(2.4777, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 4920 out of 6633 \t average loss: tensor(2.3939, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 4950 out of 6633 \t average loss: tensor(2.3745, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 4980 out of 6633 \t average loss: tensor(2.3242, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "validation accuracy =  0.5820257717500701 \t for 60609 masks on validation data\n",
            "\n",
            "Saving model checkpoint to  drive/My Drive/datasets/bert_finetune/last\n",
            "Saving optimizer and scheduler states to  drive/My Drive/datasets/bert_finetune/\n",
            "epoch: 1 out of 4 \t batch: 5010 out of 6633 \t average loss: tensor(2.3688, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 5040 out of 6633 \t average loss: tensor(2.2729, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 5070 out of 6633 \t average loss: tensor(2.5478, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 5100 out of 6633 \t average loss: tensor(2.5472, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 5130 out of 6633 \t average loss: tensor(2.2493, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 5160 out of 6633 \t average loss: tensor(2.3705, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 5190 out of 6633 \t average loss: tensor(2.4647, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 5220 out of 6633 \t average loss: tensor(2.5178, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 5250 out of 6633 \t average loss: tensor(2.5234, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 5280 out of 6633 \t average loss: tensor(2.3571, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "epoch: 1 out of 4 \t batch: 5310 out of 6633 \t average loss: tensor(2.2813, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "\n",
            "validation accuracy =  0.582017070538605 \t for 61158 masks on validation data\n",
            "\n",
            "Saving model checkpoint to  drive/My Drive/datasets/bert_finetune/last\n",
            "Saving optimizer and scheduler states to  drive/My Drive/datasets/bert_finetune/\n",
            "epoch: 1 out of 4 \t batch: 5340 out of 6633 \t average loss: tensor(2.4784, device='cuda:0', grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-a1fb6d1da232>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-c4fccb6fde91>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_dataloader, validation_dataloader, tokenizer, max_epochs, save_path, device, new_model, print_freq, val_freq)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m       \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0;31m# In-place operations to update the averages at the same time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}