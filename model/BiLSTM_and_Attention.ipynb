{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import codecs\n",
    "import itertools\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import torch.utils.data as data_utils\n",
    "import time\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from config import *\n",
    "#from torchsummary import summary\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_words_tags(file, word_ind, tag_ind, prob_ind, caseless=True):\n",
    "    \n",
    "    with codecs.open(file, 'r', 'utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    #print(lines)\n",
    "    words = []\n",
    "    tags = []\n",
    "    probs = []\n",
    "    temp_w = []\n",
    "    temp_t = []\n",
    "    temp_p = []\n",
    "    \n",
    "    for line in lines:\n",
    "        if not (line.isspace()):\n",
    "            feats = line.strip().split()\n",
    "            temp_w.append(feats[word_ind].lower() if caseless else feats[word_ind])\n",
    "            temp_t.append(feats[tag_ind])\n",
    "            temp_p.append((float)(feats[prob_ind]))\n",
    "        elif len(temp_w) > 0:\n",
    "            assert len(temp_w) == len(temp_t)\n",
    "            words.append(temp_w)\n",
    "            tags.append(temp_t)\n",
    "            probs.append(temp_p)\n",
    "            temp_w = []\n",
    "            temp_t = []\n",
    "            temp_p = []\n",
    "            \n",
    "    if len(temp_w) > 0:\n",
    "        assert len(temp_w) == len(temp_t)\n",
    "        words.append(temp_w)\n",
    "        tags.append(temp_t)\n",
    "        probs.append(temp_p)\n",
    "            \n",
    "    assert len(words) == len(tags) == len(probs)\n",
    "    #print(probs)\n",
    "    \n",
    "    return words, tags, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../datasets/train.txt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_file = \"datasets/train.txt\"\n",
    "# dev_file = \"datasets/dev.txt\"\n",
    "\n",
    "word_index = 1\n",
    "tag_index = 5\n",
    "prob_index = 4\n",
    "\n",
    "caseless=True\n",
    "\n",
    "t_words , t_tags , t_probs = read_words_tags(train_file,word_index,tag_index,prob_index,caseless)\n",
    "d_words , d_tags , d_probs = read_words_tags(dev_file,word_index,tag_index,prob_index,caseless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_maps(words, tags, min_word_freq=5, min_char_freq=1):\n",
    "    \n",
    "    word_freq = Counter()\n",
    "    char_freq = Counter()\n",
    "    tag_map = set()\n",
    "    for w, t in zip(words, tags):\n",
    "        word_freq.update(w)\n",
    "        char_freq.update(list(reduce(lambda x, y: list(x) + [' '] + list(y), w)))\n",
    "        tag_map.update(t)\n",
    "    #print(word_freq)\n",
    "    \n",
    "    word_map = {k: v + 1 for v, k in enumerate([w for w in word_freq.keys() if word_freq[w] > min_word_freq])}\n",
    "    char_map = {k: v + 1 for v, k in enumerate([c for c in char_freq.keys() if char_freq[c] > min_char_freq])}\n",
    "    tag_map = {k: v + 1 for v, k in enumerate(tag_map)}\n",
    "\n",
    "    word_map['<pad>'] = 0\n",
    "    word_map['<end>'] = len(word_map)\n",
    "    word_map['<unk>'] = len(word_map)\n",
    "    char_map['<pad>'] = 0\n",
    "    char_map['<end>'] = len(char_map)\n",
    "    char_map['<unk>'] = len(char_map)\n",
    "    tag_map['<pad>'] = 0\n",
    "    tag_map['<start>'] = len(tag_map)\n",
    "    tag_map['<end>'] = len(tag_map)\n",
    "    #print(word_map)\n",
    "    \n",
    "    return word_map, char_map, tag_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_word_freq=1\n",
    "min_char_freq=1\n",
    "\n",
    "word_map, char_map, tag_map = create_maps(t_words+d_words,t_tags+d_tags,min_word_freq, min_char_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009\n"
     ]
    }
   ],
   "source": [
    "print(len(word_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_tensors(words, tags, probs, word_map, char_map, tag_map):\n",
    "   \n",
    "    # Encode sentences into word maps with <end> at the end\n",
    "    # [['dunston', 'checks', 'in', '<end>']] -> [[4670, 4670, 185, 4669]]\n",
    "    wmaps = list(map(lambda s: list(map(lambda w: word_map.get(w, word_map['<unk>']), s)) + [word_map['<end>']], words))\n",
    "\n",
    "    # Forward and backward character streams\n",
    "    # [['d', 'u', 'n', 's', 't', 'o', 'n', ' ', 'c', 'h', 'e', 'c', 'k', 's', ' ', 'i', 'n', ' ']]\n",
    "    chars_f = list(map(lambda s: list(reduce(lambda x, y: list(x) + [' '] + list(y), s)) + [' '], words))\n",
    "    # [['n', 'i', ' ', 's', 'k', 'c', 'e', 'h', 'c', ' ', 'n', 'o', 't', 's', 'n', 'u', 'd', ' ']]\n",
    "    chars_b = list(\n",
    "        map(lambda s: list(reversed([' '] + list(reduce(lambda x, y: list(x) + [' '] + list(y), s)))), words))\n",
    "\n",
    "    # Encode streams into forward and backward character maps with <end> at the end\n",
    "    # [[29, 2, 12, 8, 7, 14, 12, 3, 6, 18, 1, 6, 21, 8, 3, 17, 12, 3, 60]]\n",
    "    cmaps_f = list(\n",
    "        map(lambda s: list(map(lambda c: char_map.get(c, char_map['<unk>']), s)) + [char_map['<end>']], chars_f))\n",
    "    # [[12, 17, 3, 8, 21, 6, 1, 18, 6, 3, 12, 14, 7, 8, 12, 2, 29, 3, 60]]\n",
    "    cmaps_b = list(\n",
    "        map(lambda s: list(map(lambda c: char_map.get(c, char_map['<unk>']), s)) + [char_map['<end>']], chars_b))\n",
    "\n",
    "    # Positions of spaces and <end> character\n",
    "    # Words are predicted or encoded at these places in the language and tagging models respectively\n",
    "    # [[7, 14, 17, 18]] are points after '...dunston', '...checks', '...in', '...<end>' respectively\n",
    "    cmarkers_f = list(map(lambda s: [ind for ind in range(len(s)) if s[ind] == char_map[' ']] + [len(s) - 1], cmaps_f))\n",
    "    # Reverse the markers for the backward stream before adding <end>, so the words of the f and b markers coincide\n",
    "    # i.e., [[17, 9, 2, 18]] are points after '...notsnud', '...skcehc', '...ni', '...<end>' respectively\n",
    "    cmarkers_b = list(\n",
    "        map(lambda s: list(reversed([ind for ind in range(len(s)) if s[ind] == char_map[' ']])) + [len(s) - 1],\n",
    "            cmaps_b))\n",
    "\n",
    "    # Encode tags into tag maps with <end> at the end\n",
    "    tmaps = list(map(lambda s: list(map(lambda t: tag_map[t], s)) + [tag_map['<end>']], tags))\n",
    "    \n",
    "    # Since we're using CRF scores of size (prev_tags, cur_tags), find indices of target sequence in the unrolled scores\n",
    "    # This will be row_index (i.e. prev_tag) * n_columns (i.e. tagset_size) + column_index (i.e. cur_tag)\n",
    "    #tmaps = list(map(lambda s: [tag_map['<start>'] * len(tag_map) + s[0]] + [s[i - 1] * len(tag_map) + s[i] for i in range(1, len(s))], tmaps))\n",
    "    # Note - the actual tag indices can be recovered with tmaps % len(tag_map)\n",
    "\n",
    "    # Pad, because need fixed length to be passed around by DataLoaders and other layers\n",
    "    word_pad_len = max(list(map(lambda s: len(s), wmaps)))\n",
    "    char_pad_len = max(list(map(lambda s: len(s), cmaps_f)))\n",
    "\n",
    "    # Sanity check\n",
    "    assert word_pad_len == max(list(map(lambda s: len(s), tmaps)))\n",
    "\n",
    "    padded_wmaps = []\n",
    "    padded_cmaps_f = []\n",
    "    padded_cmaps_b = []\n",
    "    padded_cmarkers_f = []\n",
    "    padded_cmarkers_b = []\n",
    "    padded_tmaps = []\n",
    "    wmap_lengths = []\n",
    "    cmap_lengths = []\n",
    "    padded_probs = []\n",
    "\n",
    "    for w, cf, cb, cmf, cmb, t,p in zip(wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps,probs):\n",
    "        # Sanity  checks\n",
    "        assert len(w) == len(cmf) == len(cmb) == len(t)\n",
    "        assert len(cmaps_f) == len(cmaps_b)\n",
    "\n",
    "        # Pad\n",
    "        # A note -  it doesn't really matter what we pad with, as long as it's a valid index\n",
    "        # i.e., we'll extract output at those pad points (to extract equal lengths), but never use them\n",
    "\n",
    "        padded_wmaps.append(w + [word_map['<pad>']] * (word_pad_len - len(w)))\n",
    "        padded_cmaps_f.append(cf + [char_map['<pad>']] * (char_pad_len - len(cf)))\n",
    "        padded_cmaps_b.append(cb + [char_map['<pad>']] * (char_pad_len - len(cb)))\n",
    "\n",
    "        # 0 is always a valid index to pad markers with (-1 is too but torch.gather has some issues with it)\n",
    "        padded_cmarkers_f.append(cmf + [0] * (word_pad_len - len(w)))\n",
    "        padded_cmarkers_b.append(cmb + [0] * (word_pad_len - len(w)))\n",
    "\n",
    "        padded_tmaps.append(t + [tag_map['<pad>']] * (word_pad_len - len(t)))\n",
    "        padded_probs.append(p + [0] * (word_pad_len - len(p)))\n",
    "\n",
    "        wmap_lengths.append(len(w))\n",
    "        cmap_lengths.append(len(cf))\n",
    "\n",
    "        # Sanity check\n",
    "        assert len(padded_wmaps[-1]) == len(padded_tmaps[-1]) == len(padded_cmarkers_f[-1]) == len(\n",
    "            padded_cmarkers_b[-1]) == word_pad_len == len(padded_probs[-1])\n",
    "        assert len(padded_cmaps_f[-1]) == len(padded_cmaps_b[-1]) == char_pad_len\n",
    "\n",
    "    padded_wmaps = torch.LongTensor(padded_wmaps)\n",
    "    padded_cmaps_f = torch.LongTensor(padded_cmaps_f)\n",
    "    padded_cmaps_b = torch.LongTensor(padded_cmaps_b)\n",
    "    padded_cmarkers_f = torch.LongTensor(padded_cmarkers_f)\n",
    "    padded_cmarkers_b = torch.LongTensor(padded_cmarkers_b)\n",
    "    padded_tmaps = torch.LongTensor(padded_tmaps)\n",
    "    wmap_lengths = torch.LongTensor(wmap_lengths)\n",
    "    cmap_lengths = torch.LongTensor(cmap_lengths)\n",
    "    padded_probs = torch.FloatTensor(padded_probs)\n",
    "    \n",
    "    return padded_wmaps, padded_cmaps_f, padded_cmaps_b, padded_cmarkers_f, padded_cmarkers_b, padded_tmaps, wmap_lengths, cmap_lengths , padded_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "workers = 1\n",
    "\n",
    "padded_wmaps, padded_cmaps_f, padded_cmaps_b, padded_cmarkers_f, padded_cmarkers_b, padded_tmaps, wmap_lengths, cmap_lengths , padded_probs = create_input_tensors(t_words, t_tags,t_probs, word_map, char_map, tag_map)\n",
    "#print(wmap_lengths)\n",
    "t_inputs = data_utils.TensorDataset(padded_wmaps, padded_cmaps_f, padded_cmaps_b, padded_cmarkers_f, padded_cmarkers_b, padded_tmaps, wmap_lengths, cmap_lengths , padded_probs)\n",
    "train_loader = torch.utils.data.DataLoader(t_inputs, batch_size = batch_size, shuffle=True, num_workers=workers, pin_memory=False)\n",
    "\n",
    "padded_wmaps, padded_cmaps_f, padded_cmaps_b, padded_cmarkers_f, padded_cmarkers_b, padded_tmaps, wmap_lengths, cmap_lengths , padded_probs = create_input_tensors(d_words, d_tags,d_probs, word_map, char_map, tag_map)\n",
    "d_inputs = data_utils.TensorDataset(padded_wmaps, padded_cmaps_f, padded_cmaps_b, padded_cmarkers_f, padded_cmarkers_b, padded_tmaps, wmap_lengths, cmap_lengths , padded_probs)\n",
    "val_loader = torch.utils.data.DataLoader(d_inputs, batch_size = batch_size, shuffle=True, num_workers=workers, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_embedding(input_embedding):\n",
    "    \"\"\"\n",
    "    Initialize embedding tensor with values from the uniform distribution.\n",
    "    :param input_embedding: embedding tensor\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    bias = np.sqrt(3.0 / input_embedding.size(1))\n",
    "    nn.init.uniform_(input_embedding, -bias, bias)\n",
    "\n",
    "def load_embeddings(emb_file, word_map, expand_vocab=True):\n",
    "    \"\"\"\n",
    "    Load pre-trained embeddings for words in the word map.\n",
    "    :param emb_file: file with pre-trained embeddings (in the GloVe format)\n",
    "    :param word_map: word map\n",
    "    :param expand_vocab: expand vocabulary of word map to vocabulary of pre-trained embeddings?\n",
    "    :return: embeddings for words in word map, (possibly expanded) word map,\n",
    "            number of words in word map that are in-corpus (subject to word frequency threshold)\n",
    "    \"\"\"\n",
    "    with open(emb_file, 'r') as f:\n",
    "        emb_len = len(f.readline().split(' ')) - 1\n",
    "\n",
    "    print(\"Embedding length is %d.\" % emb_len)\n",
    "\n",
    "    # Create tensor to hold embeddings for words that are in-corpus\n",
    "    ic_embs = torch.FloatTensor(len(word_map), emb_len)\n",
    "    init_embedding(ic_embs)\n",
    "\n",
    "    if expand_vocab:\n",
    "        print(\"You have elected to include embeddings that are out-of-corpus.\")\n",
    "        ooc_words = []\n",
    "        ooc_embs = []\n",
    "    else:\n",
    "        print(\"You have elected NOT to include embeddings that are out-of-corpus.\")\n",
    "\n",
    "    # Read embedding file\n",
    "    print(\"\\nLoading embeddings...\")\n",
    "    for line in open(emb_file, 'r',encoding=\"utf8\"):\n",
    "        line = line.split(' ')\n",
    "        emb_word = line[0]\n",
    "        embedding = list(map(lambda t: float(t), filter(lambda n: n and not n.isspace(), line[1:])))\n",
    "\n",
    "        if not expand_vocab and emb_word not in word_map:\n",
    "            continue\n",
    "\n",
    "        # If word is in train_vocab, store at the correct index (as in the word_map)\n",
    "        if emb_word in word_map:\n",
    "            ic_embs[word_map[emb_word]] = torch.FloatTensor(embedding)\n",
    "\n",
    "        # If word is in dev or test vocab, store it and its embedding into lists\n",
    "        elif expand_vocab:\n",
    "            ooc_words.append(emb_word)\n",
    "            ooc_embs.append(embedding)\n",
    "\n",
    "    lm_vocab_size = len(word_map)  # keep track of lang. model's output vocab size (no out-of-corpus words)\n",
    "\n",
    "    if expand_vocab:\n",
    "        print(\"'word_map' is being updated accordingly.\")\n",
    "        for word in ooc_words:\n",
    "            word_map[word] = len(word_map)\n",
    "        ooc_embs = torch.FloatTensor(np.asarray(ooc_embs))\n",
    "        embeddings = torch.cat([ic_embs, ooc_embs], 0)\n",
    "\n",
    "    else:\n",
    "        embeddings = ic_embs\n",
    "\n",
    "    # Sanity check\n",
    "    assert embeddings.size(0) == len(word_map)\n",
    "\n",
    "    print(\"\\nDone.\\n Embedding vocabulary: %d\\n Language Model vocabulary: %d.\\n\" % (len(word_map), lm_vocab_size))\n",
    "\n",
    "    return embeddings, word_map, lm_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding length is 300.\n",
      "You have elected to include embeddings that are out-of-corpus.\n",
      "\n",
      "Loading embeddings...\n",
      "'word_map' is being updated accordingly.\n",
      "\n",
      "Done.\n",
      " Embedding vocabulary: 400012\n",
      " Language Model vocabulary: 2009.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emb_file = glove_300\n",
    "expand_vocab = True\n",
    "word_emb_dim = 300\n",
    "\n",
    "embeddings, word_map, lm_vocab_size = load_embeddings(emb_file, word_map,expand_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400012, 300])\n",
      "2009\n"
     ]
    }
   ],
   "source": [
    "print(embeddings.size())\n",
    "print(lm_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"Attention mechanism written by Gustavo Aguilar https://github.com/gaguilar\"\"\"\n",
    "    def __init__(self,  hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.da = hidden_size\n",
    "        self.dh = hidden_size\n",
    "\n",
    "        self.W = nn.Linear(self.dh, self.da)        # (feat_dim, attn_dim)\n",
    "        self.v = nn.Linear(self.da, 1)              # (attn_dim, 1)\n",
    "\n",
    "    def forward(self, inputs, mask):\n",
    "        # Raw scores\n",
    "        u = self.v(torch.tanh(self.W(inputs)))      # (batch, seq, hidden) -> (batch, seq, attn) -> (batch, seq, 1)\n",
    "\n",
    "        # Masked softmax\n",
    "        u = u.exp().to(device)                                 # exp to calculate softmax\n",
    "        u = mask.unsqueeze(2).float().to(device) * u           # (batch, seq, 1) * (batch, seq, 1) to zerout out-of-mask numbers\n",
    "        sums = torch.sum(u, dim=1, keepdim=True)    # now we are sure only in-mask values are in sum\n",
    "        a = u / sums                                # the probability distribution only goes to in-mask values now\n",
    "\n",
    "        # Weighted vectors\n",
    "        z = inputs * a\n",
    "\n",
    "        return  z,  a.view(inputs.size(0), inputs.size(1))\n",
    "\n",
    "\n",
    "class LM_LSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, target_size, hidden_size, vocab_size, word_emb_dim, word_rnn_dim, dropout):\n",
    "        \n",
    "        super(LM_LSTM_CRF, self).__init__()\n",
    "\n",
    "        self.target_size = target_size  # this is the size of the output vocab of the tagging model\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.wordset_size = lm_vocab_size  # this is the size of the input vocab (embedding layer) of the tagging model\n",
    "        self.word_emb_dim = word_emb_dim\n",
    "        self.word_rnn_dim = word_rnn_dim\n",
    "        self.word_rnn_layers = 2\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.word_embeds = nn.Embedding(self.wordset_size, self.word_emb_dim)  # word embedding layer\n",
    "        self.word_blstm = nn.LSTM(self.word_emb_dim, self.word_rnn_dim, num_layers=self.word_rnn_layers, bidirectional=True, dropout=dropout)  # word BLSTM\n",
    "        \n",
    "        self.attention = Attention(self.word_rnn_dim*2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.word_rnn_dim*2, self.hidden_size)  \n",
    "        self.fc2 = nn.Linear(self.hidden_size, self.target_size)\n",
    "        \n",
    "    def init_word_embeddings(self, embeddings):\n",
    "        \"\"\"\n",
    "        Initialize embeddings with pre-trained embeddings.\n",
    "        :param embeddings: pre-trained embeddings\n",
    "        \"\"\"\n",
    "        self.word_embeds.weight = nn.Parameter(embeddings)\n",
    "\n",
    "    def fine_tune_word_embeddings(self, fine_tune=False):\n",
    "        \"\"\"\n",
    "        Fine-tune embedding layer? (Not fine-tuning only makes sense if using pre-trained embeddings).\n",
    "        :param fine_tune: Fine-tune?\n",
    "        \"\"\"\n",
    "        for p in self.word_embeds.parameters():\n",
    "            p.requires_grad = fine_tune\n",
    "\n",
    "    def forward(self, wmaps, probs, wmap_lengths):\n",
    "        \n",
    "        self.batch_size = wmaps.size(0)\n",
    "        self.word_pad_len = wmaps.size(1)\n",
    "\n",
    "        # Sort by decreasing true word sequence length\n",
    "        wmap_lengths, word_sort_ind = wmap_lengths.sort(dim=0, descending=True)\n",
    "        wmaps = wmaps[word_sort_ind]\n",
    "        probs = probs[word_sort_ind]\n",
    "        \n",
    "        # Embedding look-up for words\n",
    "        w = self.word_embeds(wmaps)  # (batch_size, word_pad_len, word_emb_dim)\n",
    "        w = self.dropout(w)\n",
    "\n",
    "        # Pack padded sequence\n",
    "        w = pack_padded_sequence(w, list(wmap_lengths), batch_first=True)  # packed sequence of word_emb_dim + 2 * char_rnn_dim, with real sequence lengths\n",
    "        \n",
    "        # LSTM\n",
    "        w, _ = self.word_blstm(w)  # packed sequence of word_rnn_dim, with real sequence lengths\n",
    "\n",
    "        # Unpack packed sequence\n",
    "        w, _ = pad_packed_sequence(w, batch_first=True)  # (batch_size, max_word_len_in_batch, word_rnn_dim)\n",
    "        w = self.dropout(w)\n",
    "        \n",
    "        mask = []\n",
    "        for i in range(len(wmap_lengths)):\n",
    "            mask_row = (np.concatenate([np.ones(wmap_lengths[i]),np.zeros(len(wmaps[i])-wmap_lengths[i])])).tolist()\n",
    "            mask.append(mask_row)\n",
    "        #Attention\n",
    "        att_output, att_weights = self.attention(w, torch.from_numpy(np.asarray(mask)).float())\n",
    "        \n",
    "        # fc layers\n",
    "        w = torch.relu(self.fc1(att_output)) \n",
    "        w = self.dropout(w)\n",
    "        \n",
    "        # final score\n",
    "        scores = torch.sigmoid(self.fc2(w)) \n",
    "                \n",
    "        return scores, wmaps, probs, wmap_lengths, word_sort_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LM_LSTM_CRF(\n",
      "  (dropout): Dropout(p=0.3)\n",
      "  (word_embeds): Embedding(2009, 300)\n",
      "  (word_blstm): LSTM(300, 512, num_layers=2, dropout=0.3, bidirectional=True)\n",
      "  (attention): Attention(\n",
      "    (W): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  )\n",
      "  (fc1): Linear(in_features=1024, out_features=20, bias=True)\n",
      "  (fc2): Linear(in_features=20, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "word_rnn_dim = 512\n",
    "dropout = 0.3\n",
    "fine_tune_word_embeddings = False\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "    \n",
    "model = LM_LSTM_CRF(1, 20, lm_vocab_size, word_emb_dim, word_rnn_dim, dropout).to(device)\n",
    "print(model)\n",
    "        \n",
    "model.init_word_embeddings(embeddings.to(device))  # initialize embedding layer with pre-trained embeddings\n",
    "model.fine_tune_word_embeddings(fine_tune_word_embeddings)  # fine-tune\n",
    "optimizer = optim.Adam(params=filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001)\n",
    "\n",
    "loss_fn = nn.BCELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Keeps track of most recent, average, sum, and count of a metric.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, loss_fn, optimizer, epoch, print_freq = 25):\n",
    "    \n",
    "    model.train()  # training mode enables dropout\n",
    "\n",
    "    batch_time = AverageMeter()  # forward prop. + back prop. time per batch\n",
    "    data_time = AverageMeter()  # data loading time per batch\n",
    "    losses = AverageMeter()  # cross entropy loss\n",
    "    f1s = AverageMeter()  # f1 score\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Batches\n",
    "    for i, (wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps, wmap_lengths, cmap_lengths, probs) in enumerate(train_loader):\n",
    "        \n",
    "        data_time.update(time.time() - start)\n",
    "        max_word_len = max(wmap_lengths.tolist())\n",
    "\n",
    "        # Reduce batch's padded length to maximum in-batch sequence\n",
    "        # This saves some compute on nn.Linear layers (RNNs are unaffected, since they don't compute over the pads)\n",
    "        wmaps = wmaps[:, :max_word_len].to(device)\n",
    "        probs = probs[:, :max_word_len].to(device)\n",
    "        wmap_lengths = wmap_lengths.to(device)\n",
    "        \n",
    "        \n",
    "        # Forward prop.\n",
    "        scores, wmaps_sorted, probs_sorted, wmap_lengths_sorted, _ = model(wmaps, probs, wmap_lengths)\n",
    "               \n",
    "        # We don't predict the next word at the pads or <end> tokens\n",
    "        # We will only predict at [dunston, checks, in] among [dunston, checks, in, <end>, <pad>, <pad>, ...]\n",
    "        # So, prediction lengths are word sequence lengths - 1\n",
    "        lm_lengths = wmap_lengths_sorted - 1\n",
    "        lm_lengths = lm_lengths.tolist()\n",
    "        \n",
    "        # loss\n",
    "        probs_sorted.resize_(scores.size())  \n",
    "        \n",
    "        scores = pack_padded_sequence(scores, lm_lengths, batch_first=True).data\n",
    "        targets = pack_padded_sequence(probs_sorted, lm_lengths, batch_first=True).data\n",
    "        loss = loss_fn(scores,targets)\n",
    "\n",
    "        # Back prop.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "#         grad_clip = True\n",
    "#         if grad_clip is not None:\n",
    "#             clip_gradient(optimizer, grad_clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Keep track of metrics\n",
    "        losses.update(loss.item(), sum(lm_lengths))\n",
    "        batch_time.update(time.time() - start)\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # Print training status\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data Load Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(epoch, i, len(train_loader),\n",
    "                                                          batch_time=batch_time,\n",
    "                                                          data_time=data_time, loss=losses))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/275]\tBatch Time 0.657 (0.657)\tData Load Time 0.425 (0.425)\tLoss 0.6393 (0.6393)\t\n",
      "Epoch: [0][25/275]\tBatch Time 0.038 (0.061)\tData Load Time 0.001 (0.017)\tLoss 0.6275 (0.6497)\t\n",
      "Epoch: [0][50/275]\tBatch Time 0.042 (0.050)\tData Load Time 0.001 (0.009)\tLoss 0.5836 (0.6325)\t\n",
      "Epoch: [0][75/275]\tBatch Time 0.039 (0.045)\tData Load Time 0.001 (0.006)\tLoss 0.5715 (0.6250)\t\n",
      "Epoch: [0][100/275]\tBatch Time 0.036 (0.043)\tData Load Time 0.001 (0.005)\tLoss 0.6211 (0.6174)\t\n",
      "Epoch: [0][125/275]\tBatch Time 0.037 (0.042)\tData Load Time 0.000 (0.004)\tLoss 0.5645 (0.6144)\t\n",
      "Epoch: [0][150/275]\tBatch Time 0.036 (0.042)\tData Load Time 0.001 (0.004)\tLoss 0.5765 (0.6065)\t\n",
      "Epoch: [0][175/275]\tBatch Time 0.039 (0.042)\tData Load Time 0.001 (0.003)\tLoss 0.5328 (0.6026)\t\n",
      "Epoch: [0][200/275]\tBatch Time 0.034 (0.042)\tData Load Time 0.001 (0.003)\tLoss 0.6009 (0.5973)\t\n",
      "Epoch: [0][225/275]\tBatch Time 0.038 (0.042)\tData Load Time 0.000 (0.003)\tLoss 0.5478 (0.5943)\t\n",
      "Epoch: [0][250/275]\tBatch Time 0.041 (0.041)\tData Load Time 0.001 (0.002)\tLoss 0.5112 (0.5906)\t\n",
      "\n",
      "\n",
      "Epoch: [1][0/275]\tBatch Time 0.434 (0.434)\tData Load Time 0.393 (0.393)\tLoss 0.5470 (0.5470)\t\n",
      "Epoch: [1][25/275]\tBatch Time 0.037 (0.055)\tData Load Time 0.001 (0.016)\tLoss 0.5463 (0.5509)\t\n",
      "Epoch: [1][50/275]\tBatch Time 0.038 (0.046)\tData Load Time 0.001 (0.008)\tLoss 0.5254 (0.5463)\t\n",
      "Epoch: [1][75/275]\tBatch Time 0.033 (0.043)\tData Load Time 0.001 (0.006)\tLoss 0.5855 (0.5452)\t\n",
      "Epoch: [1][100/275]\tBatch Time 0.038 (0.042)\tData Load Time 0.001 (0.005)\tLoss 0.5390 (0.5472)\t\n",
      "Epoch: [1][125/275]\tBatch Time 0.035 (0.041)\tData Load Time 0.000 (0.004)\tLoss 0.5221 (0.5482)\t\n",
      "Epoch: [1][150/275]\tBatch Time 0.034 (0.040)\tData Load Time 0.001 (0.003)\tLoss 0.6137 (0.5482)\t\n",
      "Epoch: [1][175/275]\tBatch Time 0.039 (0.040)\tData Load Time 0.000 (0.003)\tLoss 0.5300 (0.5467)\t\n",
      "Epoch: [1][200/275]\tBatch Time 0.037 (0.039)\tData Load Time 0.001 (0.003)\tLoss 0.5071 (0.5467)\t\n",
      "Epoch: [1][225/275]\tBatch Time 0.040 (0.039)\tData Load Time 0.001 (0.002)\tLoss 0.5838 (0.5446)\t\n",
      "Epoch: [1][250/275]\tBatch Time 0.037 (0.039)\tData Load Time 0.001 (0.002)\tLoss 0.5651 (0.5441)\t\n",
      "\n",
      "\n",
      "Epoch: [2][0/275]\tBatch Time 0.426 (0.426)\tData Load Time 0.389 (0.389)\tLoss 0.5954 (0.5954)\t\n",
      "Epoch: [2][25/275]\tBatch Time 0.039 (0.052)\tData Load Time 0.000 (0.016)\tLoss 0.4892 (0.5417)\t\n",
      "Epoch: [2][50/275]\tBatch Time 0.035 (0.045)\tData Load Time 0.001 (0.008)\tLoss 0.5286 (0.5361)\t\n",
      "Epoch: [2][75/275]\tBatch Time 0.039 (0.043)\tData Load Time 0.001 (0.006)\tLoss 0.5487 (0.5380)\t\n",
      "Epoch: [2][100/275]\tBatch Time 0.038 (0.041)\tData Load Time 0.001 (0.005)\tLoss 0.5333 (0.5371)\t\n",
      "Epoch: [2][125/275]\tBatch Time 0.041 (0.041)\tData Load Time 0.000 (0.004)\tLoss 0.5411 (0.5348)\t\n",
      "Epoch: [2][150/275]\tBatch Time 0.036 (0.040)\tData Load Time 0.001 (0.003)\tLoss 0.5604 (0.5352)\t\n",
      "Epoch: [2][175/275]\tBatch Time 0.035 (0.039)\tData Load Time 0.000 (0.003)\tLoss 0.4992 (0.5341)\t\n",
      "Epoch: [2][200/275]\tBatch Time 0.041 (0.039)\tData Load Time 0.001 (0.003)\tLoss 0.5343 (0.5350)\t\n",
      "Epoch: [2][225/275]\tBatch Time 0.036 (0.039)\tData Load Time 0.001 (0.002)\tLoss 0.5017 (0.5349)\t\n",
      "Epoch: [2][250/275]\tBatch Time 0.035 (0.039)\tData Load Time 0.000 (0.002)\tLoss 0.5270 (0.5349)\t\n",
      "\n",
      "\n",
      "Epoch: [3][0/275]\tBatch Time 0.428 (0.428)\tData Load Time 0.387 (0.387)\tLoss 0.5670 (0.5670)\t\n",
      "Epoch: [3][25/275]\tBatch Time 0.035 (0.052)\tData Load Time 0.000 (0.016)\tLoss 0.4607 (0.5244)\t\n",
      "Epoch: [3][50/275]\tBatch Time 0.037 (0.044)\tData Load Time 0.001 (0.008)\tLoss 0.5065 (0.5285)\t\n",
      "Epoch: [3][75/275]\tBatch Time 0.037 (0.042)\tData Load Time 0.001 (0.006)\tLoss 0.5101 (0.5281)\t\n",
      "Epoch: [3][100/275]\tBatch Time 0.038 (0.041)\tData Load Time 0.001 (0.005)\tLoss 0.5366 (0.5262)\t\n",
      "Epoch: [3][125/275]\tBatch Time 0.039 (0.040)\tData Load Time 0.000 (0.004)\tLoss 0.5183 (0.5258)\t\n",
      "Epoch: [3][150/275]\tBatch Time 0.038 (0.039)\tData Load Time 0.001 (0.003)\tLoss 0.4590 (0.5280)\t\n",
      "Epoch: [3][175/275]\tBatch Time 0.037 (0.039)\tData Load Time 0.001 (0.003)\tLoss 0.5368 (0.5289)\t\n",
      "Epoch: [3][200/275]\tBatch Time 0.037 (0.039)\tData Load Time 0.000 (0.003)\tLoss 0.5301 (0.5289)\t\n",
      "Epoch: [3][225/275]\tBatch Time 0.036 (0.039)\tData Load Time 0.001 (0.002)\tLoss 0.5978 (0.5293)\t\n",
      "Epoch: [3][250/275]\tBatch Time 0.038 (0.038)\tData Load Time 0.001 (0.002)\tLoss 0.5127 (0.5298)\t\n",
      "\n",
      "\n",
      "Epoch: [4][0/275]\tBatch Time 0.433 (0.433)\tData Load Time 0.393 (0.393)\tLoss 0.5816 (0.5816)\t\n",
      "Epoch: [4][25/275]\tBatch Time 0.037 (0.052)\tData Load Time 0.000 (0.016)\tLoss 0.5451 (0.5423)\t\n",
      "Epoch: [4][50/275]\tBatch Time 0.038 (0.044)\tData Load Time 0.001 (0.008)\tLoss 0.5812 (0.5308)\t\n",
      "Epoch: [4][75/275]\tBatch Time 0.043 (0.042)\tData Load Time 0.001 (0.006)\tLoss 0.5279 (0.5291)\t\n",
      "Epoch: [4][100/275]\tBatch Time 0.037 (0.041)\tData Load Time 0.001 (0.005)\tLoss 0.4936 (0.5282)\t\n",
      "Epoch: [4][125/275]\tBatch Time 0.036 (0.040)\tData Load Time 0.001 (0.004)\tLoss 0.5343 (0.5294)\t\n",
      "Epoch: [4][150/275]\tBatch Time 0.044 (0.040)\tData Load Time 0.001 (0.003)\tLoss 0.5179 (0.5269)\t\n",
      "Epoch: [4][175/275]\tBatch Time 0.041 (0.039)\tData Load Time 0.001 (0.003)\tLoss 0.5459 (0.5262)\t\n",
      "Epoch: [4][200/275]\tBatch Time 0.040 (0.039)\tData Load Time 0.001 (0.003)\tLoss 0.4980 (0.5259)\t\n",
      "Epoch: [4][225/275]\tBatch Time 0.034 (0.039)\tData Load Time 0.001 (0.002)\tLoss 0.5097 (0.5248)\t\n",
      "Epoch: [4][250/275]\tBatch Time 0.037 (0.039)\tData Load Time 0.000 (0.002)\tLoss 0.4691 (0.5236)\t\n",
      "\n",
      "\n",
      "Epoch: [5][0/275]\tBatch Time 0.440 (0.440)\tData Load Time 0.403 (0.403)\tLoss 0.5269 (0.5269)\t\n",
      "Epoch: [5][25/275]\tBatch Time 0.038 (0.052)\tData Load Time 0.000 (0.016)\tLoss 0.5493 (0.5262)\t\n",
      "Epoch: [5][50/275]\tBatch Time 0.037 (0.045)\tData Load Time 0.000 (0.008)\tLoss 0.5753 (0.5247)\t\n",
      "Epoch: [5][75/275]\tBatch Time 0.032 (0.042)\tData Load Time 0.000 (0.006)\tLoss 0.5062 (0.5226)\t\n",
      "Epoch: [5][100/275]\tBatch Time 0.037 (0.041)\tData Load Time 0.001 (0.005)\tLoss 0.5723 (0.5216)\t\n",
      "Epoch: [5][125/275]\tBatch Time 0.039 (0.040)\tData Load Time 0.001 (0.004)\tLoss 0.5789 (0.5237)\t\n",
      "Epoch: [5][150/275]\tBatch Time 0.037 (0.040)\tData Load Time 0.000 (0.003)\tLoss 0.5314 (0.5241)\t\n",
      "Epoch: [5][175/275]\tBatch Time 0.030 (0.039)\tData Load Time 0.000 (0.003)\tLoss 0.5785 (0.5254)\t\n",
      "Epoch: [5][200/275]\tBatch Time 0.034 (0.039)\tData Load Time 0.001 (0.003)\tLoss 0.5358 (0.5258)\t\n",
      "Epoch: [5][225/275]\tBatch Time 0.042 (0.039)\tData Load Time 0.001 (0.002)\tLoss 0.5068 (0.5254)\t\n",
      "Epoch: [5][250/275]\tBatch Time 0.038 (0.039)\tData Load Time 0.000 (0.002)\tLoss 0.5235 (0.5236)\t\n",
      "\n",
      "\n",
      "Epoch: [6][0/275]\tBatch Time 0.436 (0.436)\tData Load Time 0.392 (0.392)\tLoss 0.5070 (0.5070)\t\n",
      "Epoch: [6][25/275]\tBatch Time 0.035 (0.052)\tData Load Time 0.001 (0.016)\tLoss 0.4936 (0.5237)\t\n",
      "Epoch: [6][50/275]\tBatch Time 0.034 (0.045)\tData Load Time 0.000 (0.008)\tLoss 0.4609 (0.5264)\t\n",
      "Epoch: [6][75/275]\tBatch Time 0.037 (0.042)\tData Load Time 0.001 (0.006)\tLoss 0.5235 (0.5234)\t\n",
      "Epoch: [6][100/275]\tBatch Time 0.035 (0.041)\tData Load Time 0.000 (0.005)\tLoss 0.4975 (0.5220)\t\n",
      "Epoch: [6][125/275]\tBatch Time 0.036 (0.040)\tData Load Time 0.000 (0.004)\tLoss 0.5580 (0.5202)\t\n",
      "Epoch: [6][150/275]\tBatch Time 0.038 (0.039)\tData Load Time 0.001 (0.003)\tLoss 0.4768 (0.5208)\t\n",
      "Epoch: [6][175/275]\tBatch Time 0.036 (0.039)\tData Load Time 0.001 (0.003)\tLoss 0.4991 (0.5205)\t\n",
      "Epoch: [6][200/275]\tBatch Time 0.040 (0.039)\tData Load Time 0.001 (0.003)\tLoss 0.5417 (0.5212)\t\n",
      "Epoch: [6][225/275]\tBatch Time 0.043 (0.039)\tData Load Time 0.000 (0.002)\tLoss 0.5167 (0.5208)\t\n",
      "Epoch: [6][250/275]\tBatch Time 0.039 (0.039)\tData Load Time 0.001 (0.002)\tLoss 0.5079 (0.5201)\t\n",
      "\n",
      "\n",
      "Epoch: [7][0/275]\tBatch Time 0.453 (0.453)\tData Load Time 0.416 (0.416)\tLoss 0.4858 (0.4858)\t\n",
      "Epoch: [7][25/275]\tBatch Time 0.037 (0.053)\tData Load Time 0.001 (0.017)\tLoss 0.5592 (0.5141)\t\n",
      "Epoch: [7][50/275]\tBatch Time 0.038 (0.046)\tData Load Time 0.001 (0.009)\tLoss 0.5462 (0.5202)\t\n",
      "Epoch: [7][75/275]\tBatch Time 0.036 (0.043)\tData Load Time 0.000 (0.006)\tLoss 0.5251 (0.5176)\t\n",
      "Epoch: [7][100/275]\tBatch Time 0.037 (0.041)\tData Load Time 0.000 (0.005)\tLoss 0.4816 (0.5173)\t\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(0, epochs):\n",
    "\n",
    "        # One epoch's training\n",
    "        train(train_loader, model, loss_fn, optimizer, epoch)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4642857142857143, 0.6724137931034483, 0.7481060606060607, 0.7878787878787878]\n"
     ]
    }
   ],
   "source": [
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    "\n",
    "def fix_padding(scores_numpy, label_probs,  mask_numpy):\n",
    "    #if len(scores_numpy) != len(mask_numpy):\n",
    "    #    print(\"Error: len(scores_numpy) != len(mask_numpy)\")\n",
    "    #assert len(scores_numpy) == len(mask_numpy)\n",
    "    #if len(label_probs) != len(mask_numpy):\n",
    "    #    print(\"len(label_probs) != len(mask_numpy)\")\n",
    "    #assert len(label_probs) == len(mask_numpy)\n",
    "\n",
    "    all_scores_no_padd = []\n",
    "    all_labels_no_pad = []\n",
    "    for i in range(len(mask_numpy)):\n",
    "        all_scores_no_padd.append(scores_numpy[i][:mask_numpy[i]])\n",
    "        all_labels_no_pad.append(label_probs[i][:mask_numpy[i]])\n",
    "\n",
    "    assert len(all_scores_no_padd) == len(all_labels_no_pad)\n",
    "    return all_scores_no_padd, all_labels_no_pad\n",
    "\n",
    "def match_M(batch_scores_no_padd, batch_labels_no_pad):\n",
    "\n",
    "    top_m = [1, 2, 3, 4]\n",
    "    batch_num_m=[]\n",
    "    batch_score_m=[]\n",
    "    for m in top_m:\n",
    "        intersects_lst = []\n",
    "        # exact_lst = []\n",
    "        score_lst = []\n",
    "        ############################################### computing scores:\n",
    "        for s in batch_scores_no_padd:\n",
    "            if len(s) <=m:\n",
    "                continue\n",
    "            h = m\n",
    "            # if len(s) > h:\n",
    "            #     while (s[np.argsort(s)[-h]] == s[np.argsort(s)[-(h + 1)]] and h < (len(s) - 1)):\n",
    "            #         h += 1\n",
    "\n",
    "            s = np.asarray(s.cpu())\n",
    "            #ind_score = np.argsort(s)[-h:]\n",
    "            ind_score = sorted(range(len(s)), key = lambda sub: s[sub])[-h:]\n",
    "            score_lst.append(ind_score)\n",
    "\n",
    "        ############################################### computing labels:\n",
    "        label_lst = []\n",
    "        for l in batch_labels_no_pad:\n",
    "            if len(l) <=m:\n",
    "                continue\n",
    "            # if it contains several top values with the same amount\n",
    "            h = m\n",
    "            l = l.cpu()\n",
    "            if len(l) > h:\n",
    "                while (l[np.argsort(l)[-h]] == l[np.argsort(l)[-(h + 1)]] and h < (len(l) - 1)):\n",
    "                    h += 1\n",
    "            l = np.asarray(l)\n",
    "            ind_label = np.argsort(l)[-h:]\n",
    "            label_lst.append(ind_label)\n",
    "\n",
    "        ############################################### :\n",
    "\n",
    "        for i in range(len(score_lst)):\n",
    "            intersect = intersection(score_lst[i], label_lst[i])\n",
    "            intersects_lst.append((len(intersect))/(min(m, len(score_lst[i]))))\n",
    "            # sorted_score_lst = sorted(score_lst[i])\n",
    "            # sorted_label_lst =  sorted(label_lst[i])\n",
    "            # if sorted_score_lst==sorted_label_lst:\n",
    "            #     exact_lst.append(1)\n",
    "            # else:\n",
    "            #     exact_lst.append(0)\n",
    "        batch_num_m.append(len(score_lst))\n",
    "        batch_score_m.append(sum(intersects_lst))\n",
    "    return batch_num_m, batch_score_m\n",
    "\n",
    "\n",
    "#Validation\n",
    "num_m = [0, 0, 0, 0]\n",
    "score_m = [0, 0, 0, 0]\n",
    "with torch.no_grad():\n",
    "    for i, (wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps, wmap_lengths, cmap_lengths, probs) in enumerate(val_loader):\n",
    "\n",
    "\n",
    "            max_word_len = max(wmap_lengths.tolist())\n",
    "\n",
    "            # Reduce batch's padded length to maximum in-batch sequence\n",
    "            # This saves some compute on nn.Linear layers (RNNs are unaffected, since they don't compute over the pads)\n",
    "            wmaps = wmaps[:, :max_word_len].to(device)\n",
    "            probs = probs[:, :max_word_len].to(device)\n",
    "            wmap_lengths = wmap_lengths.to(device)\n",
    "\n",
    "\n",
    "            # Forward prop.\n",
    "            scores, wmaps_sorted, probs_sorted, wmap_lengths_sorted, _ = model(wmaps, probs, wmap_lengths)\n",
    "            lm_lengths = wmap_lengths_sorted - 1\n",
    "            lm_lengths = lm_lengths.tolist()\n",
    "            batch_scores_no_padd, batch_labels_no_pad = fix_padding(scores, probs_sorted, lm_lengths)\n",
    "            #print(type(batch_scores_no_padd))\n",
    "            batch_num_m, batch_score_m = match_M(batch_scores_no_padd, batch_labels_no_pad)\n",
    "            num_m = [sum(i) for i in zip(num_m, batch_num_m)]\n",
    "            score_m = [sum(i) for i in zip(score_m, batch_score_m)]\n",
    "            \n",
    "    m_score = [i/j for i,j in zip(score_m, num_m)]\n",
    "    print(m_score)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
