{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/SahilDhull/emphasis_selection/blob/master/model/Bert_and_Classifier_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 790
    },
    "colab_type": "code",
    "id": "iFZ3HHuy1mx0",
    "outputId": "d3c58fb9-f495-489c-e0a3-ce36c294bb3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/fc/bd726a15ab2c66dc09306689d04da07a3770dad724f0883f0a4bfb745087/transformers-2.4.1-py3-none-any.whl (475kB)\n",
      "\r",
      "\u001b[K     |▊                               | 10kB 31.3MB/s eta 0:00:01\r",
      "\u001b[K     |█▍                              | 20kB 6.1MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 30kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |██▊                             | 40kB 5.8MB/s eta 0:00:01\r",
      "\u001b[K     |███▍                            | 51kB 6.1MB/s eta 0:00:01\r",
      "\u001b[K     |████▏                           | 61kB 7.3MB/s eta 0:00:01\r",
      "\u001b[K     |████▉                           | 71kB 8.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████▌                          | 81kB 7.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████▏                         | 92kB 7.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████▉                         | 102kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 112kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████▎                       | 122kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 133kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▋                      | 143kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▎                     | 153kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 163kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▊                    | 174kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▍                   | 184kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 194kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 204kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▌                 | 215kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 225kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▉                | 235kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▌               | 245kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▏              | 256kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 266kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 276kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▎            | 286kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 296kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▋           | 307kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▍          | 317kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 327kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▊         | 337kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 348kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 358kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▉       | 368kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 378kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 389kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 399kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 409kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▎   | 419kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 430kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▋  | 440kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▎ | 450kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 460kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▊| 471kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 481kB 8.7MB/s \n",
      "\u001b[?25hCollecting tokenizers==0.0.11\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/36/7af38d572c935f8e0462ec7b4f7a46d73a2b3b1a938f50a5e8132d5b2dc5/tokenizers-0.0.11-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1MB 102kB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
      "\u001b[K     |████████████████████████████████| 870kB 51.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 61.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.10)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.2)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.15.0,>=1.14.10 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.10)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.10->boto3->transformers) (2.6.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.10->boto3->transformers) (0.15.2)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=1624154748c0805816a8eda484be765db1ab377b5d57d7d367bbf643b8aff700\n",
      "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
      "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.0.11 transformers-2.4.1\n",
      "Collecting config\n",
      "  Downloading https://files.pythonhosted.org/packages/59/6c/4ab0d80b22dca3baab49670b75ae2183b59649e9f27c11018075e509048e/config-0.4.2.tar.gz\n",
      "Building wheels for collected packages: config\n",
      "  Building wheel for config (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for config: filename=config-0.4.2-cp36-none-any.whl size=15135 sha256=894a40d99c2475e9914015f34f0559837f66f8b8c886e974c9458b7e4f4cb5fa\n",
      "  Stored in directory: /root/.cache/pip/wheels/51/7d/db/0e38d2ec57843d00cc39f8df3686984ccec689694f7bc78a38\n",
      "Successfully built config\n",
      "Installing collected packages: config\n",
      "Successfully installed config-0.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "colab_type": "code",
    "id": "oZkDAFPU1ps2",
    "outputId": "07f96b05-72f2-4fe6-e982-77454ff331da"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers import BertForMaskedLM , BertModel ,WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer , BertPreTrainedModel\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "from torch.nn.utils.rnn import pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nhNFcgUa1w1k",
    "outputId": "faece7d4-9e85-48a2-8b89-13183594deb1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla P100-PCIE-16GB'"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "kmLIQdgZ1zFP",
    "outputId": "2d46ec60-b99f-41cd-8466-5a53f2ba6912"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "train_file = 'drive/My Drive/datasets/train.txt'\n",
    "dev_file = 'drive/My Drive/datasets/dev.txt'\n",
    "\n",
    "quotes_file = 'drive/My Drive/datasets/all_quotes.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "b6b5c0db747b44cd82898f363a9c3885",
      "b12602bae0d44868b0a2c30a35acfb07",
      "1fd60470b92844fba5c0867d6dfc35fc",
      "181507bc5073481eadf9f130ff5e4e48",
      "ec02579dcd1a4b5195ae3a06d210c6fe",
      "8dedab3c580f41898a9e2efeba1fe622",
      "4df7925172984f7f989a0bc7b2020b2d",
      "1db1e2ed49504b1c932149b22d87e0ac"
     ]
    },
    "colab_type": "code",
    "id": "IplzxoD111cP",
    "outputId": "f260a96c-c7b5-499d-e1c4-b8353b134600"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b5c0db747b44cd82898f363a9c3885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=213450, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RM2bAwze2Ezt"
   },
   "outputs": [],
   "source": [
    "def read_token_map(file,word_index = 1,prob_index = 4, caseless = False):\n",
    "  \n",
    "  with codecs.open(file, 'r', 'utf-8') as f:\n",
    "      lines = f.readlines()\n",
    "\n",
    "  tokenized_texts = []\n",
    "  token_map = []\n",
    "  token_labels = []\n",
    "  sent_length = []\n",
    "\n",
    "  bert_tokens = []\n",
    "  orig_to_tok_map = []\n",
    "  labels = []\n",
    "\n",
    "  bert_tokens.append(\"[CLS]\")\n",
    "  \n",
    "  for line in lines:\n",
    "    if not (line.isspace()):\n",
    "      feats = line.strip().split()\n",
    "      word = feats[word_index].lower() if caseless else feats[word_index]\n",
    "      label = feats[prob_index].lower() if caseless else feats[prob_index]\n",
    "      labels.append((float)(label))\n",
    "      orig_to_tok_map.append(len(bert_tokens))\n",
    "      \n",
    "      if(word == \"n't\"):\n",
    "        word = \"'t\"\n",
    "        if(bert_tokens[-1] != \"won\"):\n",
    "          bert_tokens[-1] = bert_tokens[-1] +\"n\"\n",
    "      if(word == \"wo\"):\n",
    "        word == \"won\"\n",
    "\n",
    "      bert_tokens.extend(tokenizer.tokenize(word))\n",
    "     \n",
    "    elif len(orig_to_tok_map) > 0:\n",
    "      bert_tokens.append(\"[SEP]\")\n",
    "      tokenized_texts.append(bert_tokens)\n",
    "      token_map.append(orig_to_tok_map)\n",
    "      token_labels.append(labels)\n",
    "      sent_length.append(len(labels))\n",
    "      bert_tokens = []\n",
    "      orig_to_tok_map = []\n",
    "      labels = []\n",
    "      length = 0\n",
    "      bert_tokens.append(\"[CLS]\")\n",
    "          \n",
    "  if len(orig_to_tok_map) > 0:\n",
    "    bert_tokens.append(\"[SEP]\")\n",
    "    tokenized_texts.append(bert_tokens)\n",
    "    token_map.append(orig_to_tok_map)\n",
    "    token_labels.append(labels)\n",
    "    sent_length.append(len(labels))\n",
    "  \n",
    "  return tokenized_texts, token_map, token_labels, sent_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "colab_type": "code",
    "id": "IflGdsPR2LmX",
    "outputId": "3cd0f68a-7f55-4002-ed16-aa0ca835013c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Happiness', 'consists', 'in', 'realizing', 'it', 'is', 'all', 'a', 'great', 'strange', 'dream', '.', '[SEP]']\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "[0.6666666666666666, 0.1111111111111111, 0.0, 0.2222222222222222, 0.0, 0.1111111111111111, 0.1111111111111111, 0.0, 0.2222222222222222, 0.3333333333333333, 0.3333333333333333, 0.1111111111111111]\n",
      "12\n",
      "['[CLS]', '`', '`', 'F', '##as', '##cin', '##ating', 'social', 'media', 'tip', 'or', 'fact', 'to', 'share', '.', \"'\", \"'\", '@', 'Speaker', 'Name', '[SEP]']\n",
      "[1, 3, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 19]\n",
      "[0.0, 0.5555555555555556, 0.0, 0.1111111111111111, 0.2222222222222222, 0.1111111111111111, 0.1111111111111111, 0.0, 0.2222222222222222, 0.0, 0.0, 0.2222222222222222, 0.2222222222222222]\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "t_tokenized_texts, t_token_map, t_token_label, t_sent_length = read_token_map(train_file)\n",
    "print(t_tokenized_texts[100])\n",
    "print(t_token_map[100])\n",
    "print(t_token_label[100])\n",
    "print(t_sent_length[100])\n",
    "\n",
    "d_tokenized_texts, d_token_map, d_token_label, d_sent_length = read_token_map(dev_file)\n",
    "print(d_tokenized_texts[50])\n",
    "print(d_token_map[50])\n",
    "print(d_token_label[50])\n",
    "print(d_sent_length[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "id": "Q8IsOd0i2PnP",
    "outputId": "0becb982-ffb2-48f1-9127-f0a2c88dd5a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  101 25410  2923  1107 10459  1122  1110  1155   170  1632  4020  4185\n",
      "   119   102     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0]\n",
      "[0.66666667 0.11111111 0.         0.22222222 0.         0.11111111\n",
      " 0.11111111 0.         0.22222222 0.33333333 0.33333333 0.11111111\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 52\n",
    "\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "t_input_ids = [tokenizer.convert_tokens_to_ids(x) for x in t_tokenized_texts]\n",
    "\n",
    "# Pad our input tokens\n",
    "t_input_ids = pad_sequences(t_input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "t_token_map = pad_sequences(t_token_map, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "t_token_label = pad_sequences(t_token_label, maxlen=MAX_LEN, dtype=\"float\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "print(t_input_ids[100])\n",
    "print(t_token_map[100])\n",
    "print(t_token_label[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "id": "1Sllf5eX2TJg",
    "outputId": "30328301-a602-4d89-e35e-551a407bd78f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  101   169   169   143  2225 16430  3798  1934  2394  5580  1137  1864\n",
      "  1106  2934   119   112   112   137  9911 10208   102     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "[ 1  3  7  8  9 10 11 12 13 14 15 17 19  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0]\n",
      "[0.         0.55555556 0.         0.11111111 0.22222222 0.11111111\n",
      " 0.11111111 0.         0.22222222 0.         0.         0.22222222\n",
      " 0.22222222 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "d_input_ids = [tokenizer.convert_tokens_to_ids(x) for x in d_tokenized_texts]\n",
    "\n",
    "# Pad our input tokens\n",
    "d_input_ids = pad_sequences(d_input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "d_token_map = pad_sequences(d_token_map, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "d_token_label = pad_sequences(d_token_label, maxlen=MAX_LEN, dtype=\"float\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "print(d_input_ids[50])\n",
    "print(d_token_map[50])\n",
    "print(d_token_label[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "GWqYZ8vV2WYx",
    "outputId": "5bbfbb71-b9b1-4877-889d-e06620ef01b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "t_attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in t_input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  t_attention_masks.append(seq_mask)\n",
    "print(t_attention_masks[100])\n",
    "\n",
    "d_attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in d_input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  d_attention_masks.append(seq_mask)\n",
    "print(d_attention_masks[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GtnslThr2ZnQ"
   },
   "outputs": [],
   "source": [
    "t_input_ids = torch.tensor(t_input_ids)\n",
    "t_token_map = torch.tensor(t_token_map )\n",
    "t_token_label = torch.tensor(t_token_label)\n",
    "t_attention_masks = torch.tensor(t_attention_masks)\n",
    "t_sent_length = torch.tensor(t_sent_length)\n",
    "\n",
    "d_input_ids = torch.tensor(d_input_ids)\n",
    "d_token_map = torch.tensor(d_token_map )\n",
    "d_token_label = torch.tensor(d_token_label)\n",
    "d_attention_masks = torch.tensor(d_attention_masks)\n",
    "d_sent_length = torch.tensor(d_sent_length)\n",
    "\n",
    "# Select a batch size for training. \n",
    "batch_size = 32\n",
    "# print(t_token_labels)\n",
    "# Create an iterator of our data with torch DataLoader \n",
    "train_data = TensorDataset(t_input_ids, t_token_map, t_token_label, t_attention_masks, t_sent_length)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "validation_data = TensorDataset(d_input_ids, d_token_map, d_token_label, d_attention_masks, d_sent_length)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7-F1P6at2kGa"
   },
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    "\n",
    "def fix_padding(scores_numpy, label_probs,  mask_numpy):\n",
    "    #if len(scores_numpy) != len(mask_numpy):\n",
    "    #    print(\"Error: len(scores_numpy) != len(mask_numpy)\")\n",
    "    #assert len(scores_numpy) == len(mask_numpy)\n",
    "    #if len(label_probs) != len(mask_numpy):\n",
    "    #    print(\"len(label_probs) != len(mask_numpy)\")\n",
    "    #assert len(label_probs) == len(mask_numpy)\n",
    "\n",
    "    all_scores_no_padd = []\n",
    "    all_labels_no_pad = []\n",
    "    for i in range(len(mask_numpy)):\n",
    "        all_scores_no_padd.append(scores_numpy[i][:int(mask_numpy[i])])\n",
    "        all_labels_no_pad.append(label_probs[i][:int(mask_numpy[i])])\n",
    "\n",
    "    assert len(all_scores_no_padd) == len(all_labels_no_pad)\n",
    "    return all_scores_no_padd, all_labels_no_pad\n",
    "\n",
    "def match_M(batch_scores_no_padd, batch_labels_no_pad):\n",
    "\n",
    "    top_m = [1, 2, 3, 4]\n",
    "    batch_num_m=[]\n",
    "    batch_score_m=[]\n",
    "    for m in top_m:\n",
    "        intersects_lst = []\n",
    "        # exact_lst = []\n",
    "        score_lst = []\n",
    "        ############################################### computing scores:\n",
    "        for s in batch_scores_no_padd:\n",
    "            if len(s) <=m:\n",
    "                continue\n",
    "            h = m\n",
    "            # if len(s) > h:\n",
    "            #     while (s[np.argsort(s)[-h]] == s[np.argsort(s)[-(h + 1)]] and h < (len(s) - 1)):\n",
    "            #         h += 1\n",
    "\n",
    "            # s = np.asarray(s.cpu())\n",
    "            s = np.asarray(s)\n",
    "            #ind_score = np.argsort(s)[-h:]\n",
    "            ind_score = sorted(range(len(s)), key = lambda sub: s[sub])[-h:]\n",
    "            score_lst.append(ind_score)\n",
    "\n",
    "        ############################################### computing labels:\n",
    "        label_lst = []\n",
    "        for l in batch_labels_no_pad:\n",
    "            if len(l) <=m:\n",
    "                continue\n",
    "            # if it contains several top values with the same amount\n",
    "            h = m\n",
    "            # l = l.cpu()\n",
    "            if len(l) > h:\n",
    "                while (l[np.argsort(l)[-h]] == l[np.argsort(l)[-(h + 1)]] and h < (len(l) - 1)):\n",
    "                    h += 1\n",
    "            l = np.asarray(l)\n",
    "            ind_label = np.argsort(l)[-h:]\n",
    "            label_lst.append(ind_label)\n",
    "\n",
    "        ############################################### :\n",
    "\n",
    "        for i in range(len(score_lst)):\n",
    "            intersect = intersection(score_lst[i], label_lst[i])\n",
    "            intersects_lst.append((len(intersect))/(min(m, len(score_lst[i]))))\n",
    "            # sorted_score_lst = sorted(score_lst[i])\n",
    "            # sorted_label_lst =  sorted(label_lst[i])\n",
    "            # if sorted_score_lst==sorted_label_lst:\n",
    "            #     exact_lst.append(1)\n",
    "            # else:\n",
    "            #     exact_lst.append(0)\n",
    "        batch_num_m.append(len(score_lst))\n",
    "        batch_score_m.append(sum(intersects_lst))\n",
    "    return batch_num_m, batch_score_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gRM_nU783HIc"
   },
   "outputs": [],
   "source": [
    "def validation(model, validation_dataloader):\n",
    "  print(\"\")\n",
    "  print(\"Running Validation...\")\n",
    "\n",
    "  model.eval()\n",
    "  eval_loss, eval_accuracy = 0, 0\n",
    "  nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "  num_m = [0, 0, 0, 0]\n",
    "  score_m = [0, 0, 0, 0]\n",
    "\n",
    "  # Evaluate data for one epoch\n",
    "  for batch in validation_dataloader:\n",
    "      \n",
    "      # Add batch to GPU\n",
    "      batch = tuple(t.to(device) for t in batch)\n",
    "      \n",
    "      # Unpack the inputs from our dataloader\n",
    "      v_input_ids = batch[0].to(device)\n",
    "      v_input_mask = batch[3].to(device)\n",
    "      v_token_starts = batch[1].to(device)\n",
    "      v_labels = batch[2].to(device)\n",
    "      v_sent_length = batch[4]\n",
    "            \n",
    "      # Telling the model not to compute or store gradients, saving memory and\n",
    "      # speeding up validation\n",
    "      with torch.no_grad():        \n",
    "          output = model(v_input_ids, v_input_mask, v_labels, v_token_starts, v_sent_length)\n",
    "      \n",
    "      pred_labels = output[1]\n",
    "\n",
    "      pred_labels = pred_labels.detach().cpu().numpy()\n",
    "      v_labels = v_labels.to('cpu').numpy()\n",
    "      # print(pred_labels[0])\n",
    "      # print(v_labels[0])\n",
    "      \n",
    "      pred_labels, v_labels = fix_padding(pred_labels, v_labels, v_sent_length)\n",
    "      # print(pred_labels[0])\n",
    "      # print(v_labels[0])\n",
    "\n",
    "      batch_num_m, batch_score_m = match_M(pred_labels, v_labels)\n",
    "      num_m = [sum(i) for i in zip(num_m, batch_num_m)]\n",
    "      score_m = [sum(i) for i in zip(score_m, batch_score_m)]\n",
    "  \n",
    "  m_score = [i/j for i,j in zip(score_m, num_m)]\n",
    "  print(\"Validation Accuracy: \")\n",
    "  print(m_score)\n",
    "  v_score = np.mean(m_score)\n",
    "  print(v_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ODH9sTED18Zf"
   },
   "outputs": [],
   "source": [
    "def avg(a, st, end):\n",
    "  k = a\n",
    "  # print(type(a[st]))\n",
    "  # print(a[st].item())\n",
    "  lis = []\n",
    "  # if(end.item()==0)\n",
    "  for i in range(st,end):\n",
    "    lis.append(a[i])\n",
    "  # print(str(st)+\" \"+str(end))\n",
    "  # print(lis)\n",
    "  x = torch.mean(torch.stack(lis),dim=0)\n",
    "  # print(type(x))\n",
    "  # print(x)\n",
    "  return x\n",
    "\n",
    "class bert_model(nn.Module):\n",
    "  def __init__(self, final_size, drop_prob, data_parallel=True):\n",
    "    super(bert_model, self).__init__()\n",
    "\n",
    "    config = BertConfig.from_pretrained('bert-base-cased', output_hidden_states=True)\n",
    "    bert = BertForMaskedLM.from_pretrained('bert-base-cased', config = config)\n",
    "    \n",
    "    cnt=0\n",
    "    for child in bert.bert.children():\n",
    "      cnt = cnt + 1\n",
    "      if cnt<=11:\n",
    "        for param in child.parameters():\n",
    "          param.requires_grad = False\n",
    "\n",
    "    if data_parallel:\n",
    "        self.bert = nn.DataParallel(bert)\n",
    "    else:\n",
    "        self.bert = bert\n",
    "    bert_dim = 12*768\n",
    "    hidden_dim1 = 1000\n",
    "    hidden_dim2 = 200\n",
    "    hidden_dim3 = 20\n",
    "\n",
    "    self.fc1 = nn.Linear(bert_dim, hidden_dim1)\n",
    "    self.fc2 = nn.Linear(hidden_dim1, hidden_dim3)\n",
    "    # self.fc3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
    "    self.fc4 = nn.Linear(hidden_dim3, final_size)\n",
    "    self.dropout = nn.Dropout(p=drop_prob)\n",
    "           \n",
    "  def forward(self, bert_ids, bert_mask, labels = None, bert_token_starts = None,lm_lengths = None):\n",
    "    \n",
    "    batch_size = bert_ids.size()[0]\n",
    "    pad_size = bert_ids.size()[1]\n",
    "    # print(\"batch size\",batch_size,\"\\t\\tpad_size\",pad_size)\n",
    "\n",
    "    if(bert_token_starts == None):\n",
    "      output = self.bert(bert_ids, attention_mask = bert_mask, masked_lm_labels=labels)\n",
    "      return output\n",
    "    \n",
    "    output = self.bert(bert_ids, attention_mask = bert_mask)\n",
    "\n",
    "    bert_out = output[1][0]\n",
    "    for layers in range(1,12,1):\n",
    "      bert_out = torch.cat((bert_out, output[1][layers]), dim=2)\n",
    "    \n",
    "    \n",
    "    pred_logits = torch.relu(self.fc1(self.dropout(bert_out)))\n",
    "    pred_logits = torch.relu(self.fc2(self.dropout(pred_logits)))\n",
    "    # pred_logits = torch.relu(self.fc3(self.dropout(pred_logits)))\n",
    "    pred_logits = torch.sigmoid(self.fc4(self.dropout(pred_logits)))\n",
    "    pred_logits = torch.squeeze(pred_logits,2)\n",
    "\n",
    "    pred_labels = labels.clone()\n",
    "    # print(pred_labels[1])\n",
    "    # print(\"\\n\")\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "      for w in range(pad_size):\n",
    "        if(bert_token_starts[b][w]!=0):\n",
    "          if(bert_token_starts[b][w]>=pad_size):\n",
    "            print(bert_token_starts[b])\n",
    "          else:\n",
    "            st = bert_token_starts[b][w]\n",
    "            end = bert_token_starts[b][w+1]\n",
    "            if(end==0):\n",
    "              break\n",
    "            pred_labels[b][w] = avg(pred_logits[b],st,end)\n",
    "\n",
    "    lm_lengths, lm_sort_ind = lm_lengths.sort(dim=0, descending=True)\n",
    "    scores = labels[lm_sort_ind]\n",
    "    targets = pred_labels[lm_sort_ind]\n",
    "    scores = pack_padded_sequence(scores, lm_lengths, batch_first=True).data\n",
    "    targets = pack_padded_sequence(targets, lm_lengths, batch_first=True).data\n",
    "\n",
    "    loss_fn = nn.BCELoss().to(device) \n",
    "    loss = loss_fn(targets,scores)\n",
    "\n",
    "    return loss, pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YJSJ0eki2gWU"
   },
   "outputs": [],
   "source": [
    "model = bert_model(1,0.3,True).to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-4, eps = 1e-8)\n",
    "\n",
    "epochs = 10\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "eMlk1zNN2ute",
    "outputId": "1ec84833-99d3-41a4-c766-dceb632ef268"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.45663265306122447, 0.6485411140583555, 0.7263257575757575, 0.7666666666666667]\n",
      "0.649541547840501\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.5051020408163265, 0.7108753315649867, 0.7736742424242425, 0.821969696969697]\n",
      "0.7029053279438131\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.5714285714285714, 0.7320954907161804, 0.7907196969696971, 0.8325757575757575]\n",
      "0.7317048791725517\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.5994897959183674, 0.7493368700265252, 0.7916666666666667, 0.8371212121212122]\n",
      "0.7444036361831927\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.5790816326530612, 0.7440318302387268, 0.8030303030303031, 0.8446969696969697]\n",
      "0.7427101839047652\n",
      "\n",
      "  Average training loss: 0.51488\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.576530612244898, 0.7546419098143236, 0.8087121212121212, 0.8446969696969697]\n",
      "0.7461454032420781\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6122448979591837, 0.7625994694960212, 0.8106060606060606, 0.8537878787878788]\n",
      "0.7598095767122861\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.625, 0.7572944297082228, 0.8115530303030304, 0.853030303030303]\n",
      "0.761719440760389\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6173469387755102, 0.7692307692307693, 0.8134469696969698, 0.85]\n",
      "0.7625061694258123\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6071428571428571, 0.7546419098143236, 0.8106060606060607, 0.8522727272727273]\n",
      "0.7561658887089922\n",
      "\n",
      "  Average training loss: 0.49586\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6198979591836735, 0.7599469496021221, 0.8106060606060606, 0.8537878787878788]\n",
      "0.7610597120449336\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6275510204081632, 0.7559681697612732, 0.8210227272727274, 0.8553030303030303]\n",
      "0.7649612369362986\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6403061224489796, 0.7639257294429708, 0.8229166666666667, 0.8575757575757575]\n",
      "0.7711810690335936\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6556122448979592, 0.7586206896551724, 0.8219696969696969, 0.8583333333333333]\n",
      "0.7736339912140404\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6454081632653061, 0.7785145888594165, 0.8191287878787882, 0.8545454545454545]\n",
      "0.7743992486372413\n",
      "\n",
      "  Average training loss: 0.49230\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6403061224489796, 0.7745358090185677, 0.8153409090909093, 0.8537878787878788]\n",
      "0.7709926798365838\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6632653061224489, 0.7718832891246684, 0.8172348484848485, 0.8568181818181818]\n",
      "0.777300406387537\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6658163265306123, 0.76657824933687, 0.8106060606060606, 0.8515151515151516]\n",
      "0.7736289469971735\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6658163265306123, 0.76657824933687, 0.8210227272727273, 0.8583333333333333]\n",
      "0.7779376591183857\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6607142857142857, 0.7718832891246684, 0.8219696969696971, 0.8613636363636363]\n",
      "0.7789827270430719\n",
      "\n",
      "  Average training loss: 0.48938\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6556122448979592, 0.7679045092838196, 0.8191287878787878, 0.8606060606060606]\n",
      "0.7758129006666568\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6632653061224489, 0.7612732095490716, 0.8181818181818183, 0.8598484848484849]\n",
      "0.775642204675456\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6632653061224489, 0.7692307692307693, 0.8257575757575758, 0.8606060606060606]\n",
      "0.7797149279292137\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6581632653061225, 0.7718832891246684, 0.8153409090909091, 0.8545454545454545]\n",
      "0.7749832295167887\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6709183673469388, 0.7625994694960212, 0.8200757575757578, 0.8568181818181818]\n",
      "0.7776029440592249\n",
      "\n",
      "  Average training loss: 0.48726\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6683673469387755, 0.7679045092838196, 0.8229166666666667, 0.8590909090909091]\n",
      "0.7795698579950427\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6556122448979592, 0.7679045092838196, 0.8200757575757577, 0.8590909090909091]\n",
      "0.7756708552121114\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.673469387755102, 0.7771883289124668, 0.8153409090909091, 0.8590909090909091]\n",
      "0.7812723837123468\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6785714285714286, 0.7692307692307693, 0.8229166666666667, 0.8575757575757575]\n",
      "0.7820736555111556\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.673469387755102, 0.7705570291777188, 0.8200757575757577, 0.8590909090909091]\n",
      "0.780798270899872\n",
      "\n",
      "  Average training loss: 0.48454\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6709183673469388, 0.7824933687002652, 0.8229166666666667, 0.8590909090909091]\n",
      "0.783854827951195\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.673469387755102, 0.7824933687002652, 0.8200757575757577, 0.8560606060606061]\n",
      "0.7830247800229327\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6785714285714286, 0.7771883289124668, 0.8219696969696969, 0.8590909090909091]\n",
      "0.7842050908861253\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6709183673469388, 0.7705570291777188, 0.8229166666666665, 0.8575757575757575]\n",
      "0.7804919551917704\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6658163265306123, 0.7718832891246684, 0.8219696969696971, 0.8560606060606061]\n",
      "0.778932479671396\n",
      "\n",
      "  Average training loss: 0.48253\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6632653061224489, 0.76657824933687, 0.8229166666666667, 0.8537878787878788]\n",
      "0.776637025228466\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6581632653061225, 0.7679045092838196, 0.824810606060606, 0.8583333333333333]\n",
      "0.7773029284959704\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6581632653061225, 0.7692307692307693, 0.8238636363636364, 0.8606060606060606]\n",
      "0.7779659328766471\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6658163265306123, 0.7718832891246684, 0.8267045454545453, 0.8590909090909091]\n",
      "0.7808737675501838\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6607142857142857, 0.7758620689655172, 0.8238636363636364, 0.8606060606060606]\n",
      "0.7802615129123749\n",
      "\n",
      "  Average training loss: 0.48139\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6632653061224489, 0.7785145888594165, 0.8257575757575758, 0.8606060606060606]\n",
      "0.7820358828363754\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6377551020408163, 0.7718832891246684, 0.8323863636363638, 0.8613636363636363]\n",
      "0.7758470977913712\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6581632653061225, 0.7745358090185677, 0.8238636363636364, 0.8575757575757575]\n",
      "0.778534617066021\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6632653061224489, 0.7639257294429708, 0.831439393939394, 0.8583333333333333]\n",
      "0.7792409407095368\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6683673469387755, 0.7771883289124668, 0.831439393939394, 0.8598484848484849]\n",
      "0.7842108886597803\n",
      "\n",
      "  Average training loss: 0.48014\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6632653061224489, 0.7718832891246684, 0.8295454545454546, 0.8583333333333333]\n",
      "0.7807568457814763\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6632653061224489, 0.7705570291777188, 0.8295454545454546, 0.8613636363636363]\n",
      "0.7811828565523147\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6658163265306123, 0.7718832891246684, 0.8285984848484849, 0.8598484848484849]\n",
      "0.7815366463380626\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6607142857142857, 0.7692307692307693, 0.8295454545454547, 0.8575757575757575]\n",
      "0.7792665667665668\n",
      "\n",
      "Running Validation...\n",
      "Validation Accuracy: \n",
      "[0.6607142857142857, 0.76657824933687, 0.8295454545454547, 0.8583333333333333]\n",
      "0.778792830732486\n",
      "\n",
      "  Average training loss: 0.47887\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "loss_values = []\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "            \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[3].to(device)\n",
    "        b_token_starts = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        b_sent_length = batch[4]\n",
    "\n",
    "        model.zero_grad()   \n",
    "        model.train()     \n",
    "\n",
    "        output = model(b_input_ids, b_input_mask, b_labels, b_token_starts,b_sent_length)\n",
    "        loss = output[0]\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "        if step % 20 == 0:\n",
    "          validation(model, validation_dataloader)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    \n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.5f}\".format(avg_train_loss))\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YyrVHMBg3AW4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Bert_and_Classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "181507bc5073481eadf9f130ff5e4e48": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1db1e2ed49504b1c932149b22d87e0ac",
      "placeholder": "​",
      "style": "IPY_MODEL_4df7925172984f7f989a0bc7b2020b2d",
      "value": "100% 213k/213k [00:00&lt;00:00, 833kB/s]"
     }
    },
    "1db1e2ed49504b1c932149b22d87e0ac": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1fd60470b92844fba5c0867d6dfc35fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8dedab3c580f41898a9e2efeba1fe622",
      "max": 213450,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ec02579dcd1a4b5195ae3a06d210c6fe",
      "value": 213450
     }
    },
    "4df7925172984f7f989a0bc7b2020b2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8dedab3c580f41898a9e2efeba1fe622": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b12602bae0d44868b0a2c30a35acfb07": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b6b5c0db747b44cd82898f363a9c3885": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1fd60470b92844fba5c0867d6dfc35fc",
       "IPY_MODEL_181507bc5073481eadf9f130ff5e4e48"
      ],
      "layout": "IPY_MODEL_b12602bae0d44868b0a2c30a35acfb07"
     }
    },
    "ec02579dcd1a4b5195ae3a06d210c6fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
