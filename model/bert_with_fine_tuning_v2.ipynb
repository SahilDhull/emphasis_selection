{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_with_fine_tuning_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMfwuC/SDHfgeX/ylDP7dCo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "caf7aac200104870b151766ae5d38494": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8c84174332324140852011a3b7062a83",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d62308fc9c244addb22b043df07ddcba",
              "IPY_MODEL_3d5081a5a0594e219e6ca096b845ae7b"
            ]
          }
        },
        "8c84174332324140852011a3b7062a83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d62308fc9c244addb22b043df07ddcba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3a51fdd00b9e4aacbced778f32d14b6e",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_32c04c3428ce4fc19982591f99ed8bf4"
          }
        },
        "3d5081a5a0594e219e6ca096b845ae7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bf4c3b23d1714c38aafdc9f1de751633",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 232k/232k [00:00&lt;00:00, 892kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b075118c22764d3789285626d910351d"
          }
        },
        "3a51fdd00b9e4aacbced778f32d14b6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "32c04c3428ce4fc19982591f99ed8bf4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bf4c3b23d1714c38aafdc9f1de751633": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b075118c22764d3789285626d910351d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SahilDhull/emphasis_selection/blob/master/model/bert_with_fine_tuning_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw-WHt68c4q9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "00e62b00-a838-45cc-deac-88ef91d02cde"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install config"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.4.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: tokenizers==0.0.11 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.11)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.2)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.9 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.9->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.9->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: config in /usr/local/lib/python3.6/dist-packages (0.4.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m55-6V8SdEhR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "37dacdc3-75af-4a0e-fe4c-3b11300c1176"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "from transformers import BertForMaskedLM , BertModel ,WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import PreTrainedModel, PreTrainedTokenizer , BertPreTrainedModel\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import codecs"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_Kg45-AdHcr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "127960a3-ae7f-4243-f3fc-bb09a396358d"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla T4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ThL71gXdKlj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "4d82db9e-a67a-4834-ec80-d11f877a2dbd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "train_file = 'drive/My Drive/datasets/train.txt'\n",
        "dev_file = 'drive/My Drive/datasets/dev.txt'\n",
        "\n",
        "quotes_file = 'drive/My Drive/datasets/all_quotes.txt'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7Oj5o9adYRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_sent(file, caseless = False):\n",
        "    \n",
        "    with codecs.open(file, 'r', 'utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "    #print(lines)\n",
        "    sent = \"\"\n",
        "    sents = []\n",
        "    \n",
        "    for line in lines:\n",
        "        if not (line.isspace()):\n",
        "            feats = line.strip().split()\n",
        "            word = feats[0].lower() if caseless else feats[0]\n",
        "            if(word == \"n't\"):\n",
        "              word = \"'t\"\n",
        "              sent = sent + \"n\"\n",
        "            sent = sent + \" \" + word\n",
        "        elif len(sent) > 0:\n",
        "            sents.append(sent.strip())\n",
        "            sent = \"\"\n",
        "            \n",
        "    if len(sent) > 0:\n",
        "        sents.append(sent)\n",
        "    \n",
        "    return sents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOVwHCdUdj8E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "caf7aac200104870b151766ae5d38494",
            "8c84174332324140852011a3b7062a83",
            "d62308fc9c244addb22b043df07ddcba",
            "3d5081a5a0594e219e6ca096b845ae7b",
            "3a51fdd00b9e4aacbced778f32d14b6e",
            "32c04c3428ce4fc19982591f99ed8bf4",
            "bf4c3b23d1714c38aafdc9f1de751633",
            "b075118c22764d3789285626d910351d"
          ]
        },
        "outputId": "d88a58fd-5bdb-4a18-d6c2-7849e8e73463"
      },
      "source": [
        "sentences = read_sent(quotes_file)\n",
        "print(sentences[0])\n",
        "print(sentences[100])\n",
        "\n",
        "sentences = [\"[CLS] \" + query + \" [SEP]\" for query in sentences]\n",
        "print(sentences[0])\n",
        "print(sentences[100])\n",
        "\n",
        "# Tokenize with BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case = False)\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "\n",
        "print (tokenized_texts[0])\n",
        "print (tokenized_texts[100])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "you know you 're in love when you can 't fall asleep because reality is finally better than your dreams .\n",
            "a half-read book is a half-finished love affair .\n",
            "[CLS] you know you 're in love when you can 't fall asleep because reality is finally better than your dreams . [SEP]\n",
            "[CLS] a half-read book is a half-finished love affair . [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "caf7aac200104870b151766ae5d38494",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "['[CLS]', 'you', 'know', 'you', \"'\", 're', 'in', 'love', 'when', 'you', 'can', \"'\", 't', 'fall', 'asleep', 'because', 'reality', 'is', 'finally', 'better', 'than', 'your', 'dreams', '.', '[SEP]']\n",
            "['[CLS]', 'a', 'half', '-', 'read', 'book', 'is', 'a', 'half', '-', 'finished', 'love', 'affair', '.', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63A92mIqdnZQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "e4e3fc7f-6dd3-4467-e30c-7c198567baad"
      },
      "source": [
        "MAX_LEN = 36\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "print(input_ids[0])\n",
        "print(input_ids[100])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 101 2017 2113 2017 1005 2128 1999 2293 2043 2017 2064 1005 1056 2991\n",
            " 6680 2138 4507 2003 2633 2488 2084 2115 5544 1012  102    0    0    0\n",
            "    0    0    0    0    0    0    0    0]\n",
            "[ 101 1037 2431 1011 3191 2338 2003 1037 2431 1011 2736 2293 6771 1012\n",
            "  102    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ic9QrWZ7fIhB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "e825a0e8-6421-4f8e-f7c4-f8f87e5fef2f"
      },
      "source": [
        "attention_masks = []\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)\n",
        "print(attention_masks[0])\n",
        "print(attention_masks[100])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPm_54FwfL0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_inputs, validation_inputs = train_test_split(input_ids, random_state=2018, test_size=0.1)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2018, test_size=0.1)\n",
        "                                             \n",
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "\n",
        "# Select a batch size for training. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVqKx1jmVfig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mask_tokens(inputs, tokenizer, mlm_probability = 0.15):\n",
        "    \"\"\" Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. \"\"\"\n",
        "    labels = inputs.clone()\n",
        "    # print(inputs[0])\n",
        "\n",
        "    # We sample a few tokens in each sequence for masked-LM training (with probability mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
        "    probability_matrix = torch.full(labels.shape, mlm_probability)\n",
        "    special_tokens_mask = [tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()]\n",
        "    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
        " \n",
        "    if tokenizer._pad_token is not None:\n",
        "        padding_mask = labels.eq(tokenizer.pad_token_id)\n",
        "        probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
        "\n",
        "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
        "    labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
        "\n",
        "    # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
        "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
        "    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
        "\n",
        "    # 10% of the time, we replace masked input tokens with random word\n",
        "    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
        "    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
        "    inputs[indices_random] = random_words[indices_random]\n",
        "\n",
        "    # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
        "    return inputs, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrHN1mVqeUVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader \n",
        "train_data = TensorDataset(train_inputs, train_masks)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mtR-BOWyn2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class bert_model(nn.Module):\n",
        "  def __init__(self, final_size, drop_prob, data_parallel=True):\n",
        "    super(bert_model, self).__init__()\n",
        "\n",
        "    config = BertConfig.from_pretrained('bert-base-cased', output_hidden_states=True)\n",
        "    bert = BertForMaskedLM.from_pretrained('bert-base-cased', config = config)\n",
        "    \n",
        "    if data_parallel:\n",
        "        self.bert = nn.DataParallel(bert)\n",
        "    else:\n",
        "        self.bert = bert\n",
        "    bert_dim = 768\n",
        "    \n",
        "    self.fc = nn.Linear(bert_dim, final_size)\n",
        "    self.dropout = nn.Dropout(p=drop_prob)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "           \n",
        "  def forward(self, bert_ids, bert_mask, labels = None, bert_token_starts = None):\n",
        "\n",
        "    output = self.bert(bert_ids, attention_mask = bert_mask, masked_lm_labels=labels)\n",
        "    bert_last_layer = bert_layers[2][0]\n",
        "    \n",
        "    pred_logits = self.sigmoid(self.fc(self.dropout(bert_last_layer)))\n",
        "    mask = true_labels != 0\n",
        "\n",
        "    loss_fn = nn.BCELoss().to(device) \n",
        "    loss = loss_fn(pred_logits, true_labels.float())\n",
        "\n",
        "    loss /= mask.float().sum()\n",
        "\n",
        "    if(bert_token_starts == None):\n",
        "      return output\n",
        "    return loss, pred_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGMNHPlz1MY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = bert_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gU-4T0d4Jjr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = BertConfig.from_pretrained('bert-base-cased', output_hidden_states=True)\n",
        "\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-cased', config = config)\n",
        "model = model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHyzgYhKG3WC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_epochs = 4\n",
        "max_steps = len(train_dataloader)*max_epochs\n",
        "\n",
        "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "optimizer_grouped_parameters = [\n",
        "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
        "    ]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters,lr = 1e-5)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=max_steps/20, num_training_steps=max_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcGSLSve3DJI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, tokenizer, validation_dataloader):\n",
        "  total_loss = 0\n",
        "  steps = len(validation_dataloader)\n",
        "  model.eval()\n",
        "  \n",
        "  corrects = 0\n",
        "  errors = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i, (val_inputs, val_masks) in enumerate(validation_dataloader):\n",
        "      inputs, labels = mask_tokens(val_inputs, tokenizer)\n",
        "      \n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      val_masks = val_masks.to(device)\n",
        "      \n",
        "      output = model(inputs,attention_mask = val_masks)\n",
        "      predict = output[0]\n",
        "\n",
        "      batch_size = predict.size()[0]\n",
        "\n",
        "      for bs in range(batch_size):\n",
        "        ii = 0\n",
        "        for ls in labels[bs]:\n",
        "          ls = ls.item()\n",
        "          if ls!=-100:\n",
        "            predicted = torch.argmax(predict[bs][ii]).item()\n",
        "            if(ls == predicted):\n",
        "              corrects = corrects + 1\n",
        "            else:\n",
        "              errors = errors + 1\n",
        "          ii = ii + 1\n",
        "\n",
        "  total = corrects + errors\n",
        "  accuracy = corrects/total\n",
        "  print(\"\\nvalidation accuracy = \",accuracy,\"\\t for\",total,\"masks on validation data\\n\")\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N4kQ8kXFOrfY",
        "colab": {}
      },
      "source": [
        "def train(train_dataloader,validation_dataloader, model, tokenizer, optimizer, scheduler, max_epochs, print_freq = 30,val_freq = 666):\n",
        "  \n",
        "  model.zero_grad()\n",
        "  steps = len(train_dataloader)\n",
        "\n",
        "  for epoch in range(max_epochs):\n",
        "    \n",
        "    # evaluate(model, tokenizer, validation_dataloader)\n",
        "    total_loss = 0\n",
        "    \n",
        "    for i, (train_inputs, train_masks) in enumerate(train_dataloader):\n",
        "      inputs, labels = mask_tokens(train_inputs, tokenizer)\n",
        "      model.train()\n",
        "      \n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      train_masks = train_masks.to(device)\n",
        "      \n",
        "      output = model(inputs,attention_mask = train_masks, masked_lm_labels=labels)\n",
        "      # print(output[2][0].size())\n",
        "\n",
        "      loss = output[0]\n",
        "      total_loss = total_loss + loss\n",
        "      \n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      model.zero_grad()\n",
        "\n",
        "      if((i+1)%print_freq==0):\n",
        "        avg_loss = total_loss/print_freq\n",
        "        total_loss = 0\n",
        "        print(\"epoch:\",(epoch+1),\"out of\",max_epochs,\"\\t batch:\",(i+1),\"out of\",steps,\"\\t average loss:\",avg_loss)   \n",
        "      \n",
        "      if((i+1)%val_freq==0):\n",
        "        evaluate(model, tokenizer, validation_dataloader)\n",
        "  \n",
        "  evaluate(model, tokenizer, validation_dataloader)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDPRSboo07YL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "02dfc33f-e2d6-465e-95f5-b0e06dba39dc"
      },
      "source": [
        "train(train_dataloader,validation_dataloader, model, tokenizer, optimizer, scheduler, max_epochs)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "13\n",
            "torch.Size([32, 36, 768])\n",
            "torch.Size([32, 36, 768])\n",
            "3\n",
            "13\n",
            "torch.Size([32, 36, 768])\n",
            "torch.Size([32, 36, 768])\n",
            "3\n",
            "13\n",
            "torch.Size([32, 36, 768])\n",
            "torch.Size([32, 36, 768])\n",
            "3\n",
            "13\n",
            "torch.Size([32, 36, 768])\n",
            "torch.Size([32, 36, 768])\n",
            "3\n",
            "13\n",
            "torch.Size([32, 36, 768])\n",
            "torch.Size([32, 36, 768])\n",
            "3\n",
            "13\n",
            "torch.Size([32, 36, 768])\n",
            "torch.Size([32, 36, 768])\n",
            "3\n",
            "13\n",
            "torch.Size([32, 36, 768])\n",
            "torch.Size([32, 36, 768])\n",
            "3\n",
            "13\n",
            "torch.Size([32, 36, 768])\n",
            "torch.Size([32, 36, 768])\n",
            "3\n",
            "13\n",
            "torch.Size([32, 36, 768])\n",
            "torch.Size([32, 36, 768])\n",
            "3\n",
            "13\n",
            "torch.Size([32, 36, 768])\n",
            "torch.Size([32, 36, 768])\n",
            "3\n",
            "13\n",
            "torch.Size([32, 36, 768])\n",
            "torch.Size([32, 36, 768])\n",
            "3\n",
            "13\n",
            "torch.Size([32, 36, 768])\n",
            "torch.Size([32, 36, 768])\n",
            "3\n",
            "13\n",
            "torch.Size([32, 36, 768])\n",
            "torch.Size([32, 36, 768])\n",
            "3\n",
            "13\n",
            "torch.Size([32, 36, 768])\n",
            "torch.Size([32, 36, 768])\n",
            "3\n",
            "13\n",
            "torch.Size([32, 36, 768])\n",
            "torch.Size([32, 36, 768])\n",
            "3\n",
            "13\n",
            "torch.Size([32, 36, 768])\n",
            "torch.Size([32, 36, 768])\n",
            "3\n",
            "13\n",
            "torch.Size([32, 36, 768])\n",
            "torch.Size([32, 36, 768])\n",
            "3\n",
            "13\n",
            "torch.Size([32, 36, 768])\n",
            "torch.Size([32, 36, 768])\n",
            "3\n",
            "13\n",
            "torch.Size([32, 36, 768])\n",
            "torch.Size([32, 36, 768])\n",
            "3\n",
            "13\n",
            "torch.Size([32, 36, 768])\n",
            "torch.Size([32, 36, 768])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-0644ccae4a80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-1739bb823fc7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_dataloader, validation_dataloader, model, tokenizer, optimizer, scheduler, max_epochs, print_freq, val_freq)\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH18KXhzXNvi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}