{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_as_embedding.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyPD8PJm7bzqXjPKl7njdF6P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SahilDhull/emphasis_selection/blob/master/model/bert_as_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ja1HKFrfoq2p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers\n",
        "!pip install config"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WhQOQhJnWA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "from transformers import BertForMaskedLM , BertModel ,WEIGHTS_NAME, AdamW\n",
        "from transformers import PreTrainedModel, PreTrainedTokenizer\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import codecs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oahAsUyroZmk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFfQPAx7pLyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "train_file = 'drive/My Drive/datasets/train.txt'\n",
        "dev_file = 'drive/My Drive/datasets/dev.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoZPpbVj0yVc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HN4W1a-0VGS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_token_map(file,word_index = 1,prob_index = 4, caseless = True):\n",
        "  \n",
        "  with codecs.open(file, 'r', 'utf-8') as f:\n",
        "      lines = f.readlines()\n",
        "\n",
        "  tokenized_texts = []\n",
        "  token_map = []\n",
        "  token_labels = []\n",
        "\n",
        "  bert_tokens = []\n",
        "  orig_to_tok_map = []\n",
        "  labels = []\n",
        "\n",
        "  bert_tokens.append(\"[CLS]\")\n",
        "  \n",
        "  for line in lines:\n",
        "    if not (line.isspace()):\n",
        "      feats = line.strip().split()\n",
        "      word = feats[word_index].lower() if caseless else feats[word_ind]\n",
        "      label = feats[prob_index].lower() if caseless else feats[word_ind]\n",
        "      labels.append((float)(label))\n",
        "      orig_to_tok_map.append(len(bert_tokens))\n",
        "      bert_tokens.extend(tokenizer.tokenize(word))\n",
        "    elif len(orig_to_tok_map) > 0:\n",
        "      bert_tokens.append(\"[SEP]\")\n",
        "      tokenized_texts.append(bert_tokens)\n",
        "      token_map.append(orig_to_tok_map)\n",
        "      token_labels.append(labels)\n",
        "      bert_tokens = []\n",
        "      orig_to_tok_map = []\n",
        "      labels = []\n",
        "      bert_tokens.append(\"[CLS]\")\n",
        "          \n",
        "  if len(orig_to_tok_map) > 0:\n",
        "    bert_tokens.append(\"[SEP]\")\n",
        "    tokenized_texts.append(bert_tokens)\n",
        "    token_map.append(orig_to_tok_map)\n",
        "    token_labels.append(labels)\n",
        "  \n",
        "  return tokenized_texts, token_map, token_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utRYZuWvHP9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_tokenized_texts, t_token_map, t_token_label = read_token_map(train_file)\n",
        "print(t_tokenized_texts[100])\n",
        "print(t_token_map[100])\n",
        "print(t_token_label[100])\n",
        "\n",
        "d_tokenized_texts, d_token_map, d_token_label = read_token_map(dev_file)\n",
        "print(d_tokenized_texts[50])\n",
        "print(d_token_map[50])\n",
        "print(d_token_label[50])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pPiNJPeKb0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 36\n",
        "\n",
        "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "t_input_ids = [tokenizer.convert_tokens_to_ids(x) for x in t_tokenized_texts]\n",
        "\n",
        "# Pad our input tokens\n",
        "t_input_ids = pad_sequences(t_input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "t_token_map = pad_sequences(t_token_map, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "t_token_label = pad_sequences(t_token_label, maxlen=MAX_LEN, dtype=\"float\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print(t_input_ids[100])\n",
        "print(t_token_map[100])\n",
        "print(t_token_label[100])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s50toXGPvCyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "d_input_ids = [tokenizer.convert_tokens_to_ids(x) for x in d_tokenized_texts]\n",
        "\n",
        "# Pad our input tokens\n",
        "d_input_ids = pad_sequences(d_input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "d_token_map = pad_sequences(d_token_map, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "d_token_label = pad_sequences(d_token_label, maxlen=MAX_LEN, dtype=\"float\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print(d_input_ids[50])\n",
        "print(d_token_map[50])\n",
        "print(d_token_label[50])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oq3lT_wYZE2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_attention_masks = []\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in t_input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  t_attention_masks.append(seq_mask)\n",
        "print(t_attention_masks[100])\n",
        "\n",
        "d_attention_masks = []\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in d_input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  d_attention_masks.append(seq_mask)\n",
        "print(d_attention_masks[50])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kT5OYV2y4nbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_inputs, validation_inputs = train_test_split(input_ids, random_state=2018, test_size=0.1)\n",
        "# train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "#                                              random_state=2018, test_size=0.1)\n",
        "                                             \n",
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "t_input_ids = torch.tensor(t_input_ids)\n",
        "t_token_map = torch.tensor(t_token_map )\n",
        "t_token_label = torch.tensor(t_token_label)\n",
        "t_attention_masks = torch.tensor(t_attention_masks)\n",
        "\n",
        "d_input_ids = torch.tensor(d_input_ids)\n",
        "d_token_map = torch.tensor(d_token_map )\n",
        "d_token_label = torch.tensor(d_token_label)\n",
        "d_attention_masks = torch.tensor(d_attention_masks)\n",
        "\n",
        "# Select a batch size for training. \n",
        "batch_size = 32\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader \n",
        "train_data = TensorDataset(t_input_ids, t_token_map, t_token_label, t_attention_masks)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "validation_data = TensorDataset(d_input_ids, d_token_map, d_token_label, d_attention_masks)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AT4YqiWZXCvk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfdoN1k4_oyL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = BertAdam(optimizer_grouped_parameters, lr=2e-5, warmup=.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB6vcoRfnhfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "# Number of training epochs \n",
        "epochs = 4\n",
        "\n",
        "# BERT training loop\n",
        "for _ in trange(epochs, desc=\"Epoch\"):  \n",
        "  \n",
        "  ## TRAINING\n",
        "  \n",
        "  # Set our model to training mode\n",
        "  model.train()  \n",
        "  # Tracking variables\n",
        "  tr_loss = 0\n",
        "  nb_tr_examples, nb_tr_steps = 0, 0\n",
        "  # Train the data for one epoch\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask = batch\n",
        "    print(b_input_ids.size(),\"\\n\\n\", b_input_mask.size())\n",
        "    # Clear out the gradients (by default they accumulate)\n",
        "    optimizer.zero_grad()\n",
        "    # Forward pass\n",
        "    hidden_rep , cls_head = model(b_input_ids, attention_mask=b_input_mask)\n",
        "    print(cls_head.size())\n",
        "    print(len(hidden_rep))\n",
        "    cls_rep = hidden_rep[0]\n",
        "\n",
        "    print(cls_rep.size())\n",
        "  #   train_loss_set.append(loss.item())    \n",
        "  #   # Backward pass\n",
        "  #   loss.backward()\n",
        "  #   # Update parameters and take a step using the computed gradient\n",
        "  #   optimizer.step()\n",
        "  #   # Update tracking variables\n",
        "  #   tr_loss += loss.item()\n",
        "  #   nb_tr_examples += b_input_ids.size(0)\n",
        "  #   nb_tr_steps += 1\n",
        "  # print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "       \n",
        "#   ## VALIDATION\n",
        "\n",
        "#   # Put model in evaluation mode\n",
        "#   model.eval()\n",
        "#   # Tracking variables \n",
        "#   eval_loss, eval_accuracy = 0, 0\n",
        "#   nb_eval_steps, nb_eval_examples = 0, 0\n",
        "#   # Evaluate data for one epoch\n",
        "#   for batch in validation_dataloader:\n",
        "#     # Add batch to GPU\n",
        "#     batch = tuple(t.to(device) for t in batch)\n",
        "#     # Unpack the inputs from our dataloader\n",
        "#     b_input_ids, b_input_mask = batch\n",
        "#     # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "#     with torch.no_grad():\n",
        "#       # Forward pass, calculate logit predictions\n",
        "#       logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)    \n",
        "#     # Move logits and labels to CPU\n",
        "#     logits = logits.detach().cpu().numpy()\n",
        "#     label_ids = b_labels.to('cpu').numpy()\n",
        "#     tmp_eval_accuracy = flat_accuracy(logits, label_ids)    \n",
        "#     eval_accuracy += tmp_eval_accuracy\n",
        "#     nb_eval_steps += 1\n",
        "#   print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
        "\n",
        "# # plot training performance\n",
        "# plt.figure(figsize=(15,8))\n",
        "# plt.title(\"Training loss\")\n",
        "# plt.xlabel(\"Batch\")\n",
        "# plt.ylabel(\"Loss\")\n",
        "# plt.plot(train_loss_set)\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDyoJ-hnwTtr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LK-BWKxYBR7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-8UNwZbBSVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}